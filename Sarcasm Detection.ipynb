{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKJINQ5yB-Vr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS6140 Project - Detection Of Sarcasm In Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxgwsNuQB-Vu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fxO2rgxTB-Vv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Download Glove"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jctNPaWvB-Vx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Run if running notebook with vectors locally for first time. Files added to gitignore\n",
    "#glove = api.load('glove-wiki-gigaword-300')\n",
    "#glove.save('glovevectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GjEaOxnKB-Vy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load('glovevectors.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GG-8VkuB-Vz",
    "outputId": "839ce457-92b2-4fa0-d72c-886fcfeeaeba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run these if not up to date\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Tokenize comments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DJ0-7mx7B-V0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "952XhfnwB-V0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data[['label', 'comment', 'subreddit', 'score', 'parent_comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "a_iBqMpFB-V0",
    "outputId": "472442a4-ddc3-48fb-d758-06b3ef5a6d72",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   label                                            comment  \\\n0      0                                         NC and NH.   \n1      0  You do know west teams play against west teams...   \n2      0  They were underdogs earlier today, but since G...   \n3      0  This meme isn't funny none of the \"new york ni...   \n4      0                    I could use one of those tools.   \n5      0  I don't pay attention to her, but as long as s...   \n6      0      Trick or treating in general is just weird...   \n7      0                    Blade Mastery+Masamune or GTFO!   \n8      0  You don't have to, you have a good build, buy ...   \n9      0                  I would love to see him at lolla.   \n\n            subreddit  score  \\\n0            politics      2   \n1                 nba     -4   \n2                 nfl      3   \n3  BlackPeopleTwitter     -8   \n4  MaddenUltimateTeam      6   \n5           AskReddit      0   \n6           AskReddit      1   \n7       FFBraveExvius      2   \n8        pcmasterrace      1   \n9        Lollapalooza      2   \n\n                                      parent_comment  \n0  Yeah, I get that argument. At this point, I'd ...  \n1  The blazers and Mavericks (The wests 5 and 6 s...  \n2                            They're favored to win.  \n3                         deadass don't kill my buzz  \n4  Yep can confirm I saw the tool they use for th...  \n5                   do you find ariana grande sexy ?  \n6  What's your weird or unsettling Trick or Treat...  \n7  Probably Sephiroth. I refuse to taint his grea...  \n8  What to upgrade? I have $500 to spend (mainly ...  \n9  Probably count Kanye out Since the rest of his...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>I don't pay attention to her, but as long as s...</td>\n      <td>AskReddit</td>\n      <td>0</td>\n      <td>do you find ariana grande sexy ?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>Trick or treating in general is just weird...</td>\n      <td>AskReddit</td>\n      <td>1</td>\n      <td>What's your weird or unsettling Trick or Treat...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>Blade Mastery+Masamune or GTFO!</td>\n      <td>FFBraveExvius</td>\n      <td>2</td>\n      <td>Probably Sephiroth. I refuse to taint his grea...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>You don't have to, you have a good build, buy ...</td>\n      <td>pcmasterrace</td>\n      <td>1</td>\n      <td>What to upgrade? I have $500 to spend (mainly ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>I would love to see him at lolla.</td>\n      <td>Lollapalooza</td>\n      <td>2</td>\n      <td>Probably count Kanye out Since the rest of his...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWFbcerEB-V2",
    "outputId": "a01c2a17-546c-45c0-a294-d5f1191e9892",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "label              0\ncomment           53\nsubreddit          0\nscore              0\nparent_comment     0\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EAjZeoulB-V2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPV_mJCcB-V3",
    "outputId": "55794c3f-4375-451c-99f0-ddf361b2f35d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "label             0\ncomment           0\nsubreddit         0\nscore             0\nparent_comment    0\ndtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K6cgEllvB-V3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['comment_tokens'] = data['comment']\n",
    "data['comment_tokens'] = data['comment_tokens'].str.lower()\n",
    "data['comment_tokens'] = data['comment_tokens'].str.replace('can\\'t','can not',regex = True)\n",
    "data['comment_tokens'] = data['comment_tokens'].str.replace('\\'d',' would',regex = True)\n",
    "data['comment_tokens'] = data['comment_tokens'].str.replace('wouldn\\'t','would not',regex = True)\n",
    "data['comment_tokens'] = data['comment_tokens'].str.replace('couldn\\'t','could not',regex = True)\n",
    "data['comment_tokens'] = data['comment_tokens'].str.replace('[^\\w\\s]','', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9ajDQKuaB-V4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['parent_comment_tokens'] = data['parent_comment']\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.lower()\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.replace('can\\'t','can not',regex = True)\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.replace('\\'d',' would',regex = True)\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.replace('wouldn\\'t','would not',regex = True)\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.replace('couldn\\'t','could not',regex = True)\n",
    "data['parent_comment_tokens'] = data['parent_comment_tokens'].str.replace('[^\\w\\s]','', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YmNKK5FIB-V4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def apply_lemmatizer(sentence):\n",
    "    return [lemma.lemmatize(token) for token in wordpunct_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "53Tz39z1B-V5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[\"comment_tokens\"] = data.comment_tokens.apply(apply_lemmatizer)\n",
    "data[\"parent_comment_tokens\"] = data.parent_comment_tokens.apply(apply_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Babfw21UB-V5",
    "outputId": "28a55c5c-453b-4145-865e-5e4511eecd0c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                             comment  \\\n0                                         NC and NH.   \n1  You do know west teams play against west teams...   \n2  They were underdogs earlier today, but since G...   \n3  This meme isn't funny none of the \"new york ni...   \n4                    I could use one of those tools.   \n5  I don't pay attention to her, but as long as s...   \n6      Trick or treating in general is just weird...   \n7                    Blade Mastery+Masamune or GTFO!   \n8  You don't have to, you have a good build, buy ...   \n9                  I would love to see him at lolla.   \n\n                                      comment_tokens  \n0                                      [nc, and, nh]  \n1  [you, do, know, west, team, play, against, wes...  \n2  [they, were, underdog, earlier, today, but, si...  \n3  [this, meme, isnt, funny, none, of, the, new, ...  \n4              [i, could, use, one, of, those, tool]  \n5  [i, dont, pay, attention, to, her, but, a, lon...  \n6  [trick, or, treating, in, general, is, just, w...  \n7                 [blade, masterymasamune, or, gtfo]  \n8  [you, dont, have, to, you, have, a, good, buil...  \n9          [i, would, love, to, see, him, at, lolla]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>comment_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NC and NH.</td>\n      <td>[nc, and, nh]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You do know west teams play against west teams...</td>\n      <td>[you, do, know, west, team, play, against, wes...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>[they, were, underdog, earlier, today, but, si...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>[this, meme, isnt, funny, none, of, the, new, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I could use one of those tools.</td>\n      <td>[i, could, use, one, of, those, tool]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I don't pay attention to her, but as long as s...</td>\n      <td>[i, dont, pay, attention, to, her, but, a, lon...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Trick or treating in general is just weird...</td>\n      <td>[trick, or, treating, in, general, is, just, w...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Blade Mastery+Masamune or GTFO!</td>\n      <td>[blade, masterymasamune, or, gtfo]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>You don't have to, you have a good build, buy ...</td>\n      <td>[you, dont, have, to, you, have, a, good, buil...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>I would love to see him at lolla.</td>\n      <td>[i, would, love, to, see, him, at, lolla]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"comment\", \"comment_tokens\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90mlFX-TB-V6",
    "outputId": "c3e07284-5b3e-4901-808c-89d2c5debb47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1010773,)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "PhMNLACvB-V6",
    "outputId": "ff40cd20-aee9-4d41-b763-6fbf3cfce398",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                      parent_comment  \\\n0  Yeah, I get that argument. At this point, I'd ...   \n1  The blazers and Mavericks (The wests 5 and 6 s...   \n2                            They're favored to win.   \n3                         deadass don't kill my buzz   \n4  Yep can confirm I saw the tool they use for th...   \n5                   do you find ariana grande sexy ?   \n6  What's your weird or unsettling Trick or Treat...   \n7  Probably Sephiroth. I refuse to taint his grea...   \n8  What to upgrade? I have $500 to spend (mainly ...   \n9  Probably count Kanye out Since the rest of his...   \n\n                               parent_comment_tokens  \n0  [yeah, i, get, that, argument, at, this, point...  \n1  [the, blazer, and, maverick, the, west, 5, and...  \n2                         [theyre, favored, to, win]  \n3                    [deadass, dont, kill, my, buzz]  \n4  [yep, can, confirm, i, saw, the, tool, they, u...  \n5              [do, you, find, ariana, grande, sexy]  \n6  [whats, your, weird, or, unsettling, trick, or...  \n7  [probably, sephiroth, i, refuse, to, taint, hi...  \n8  [what, to, upgrade, i, have, 500, to, spend, m...  \n9  [probably, count, kanye, out, since, the, rest...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parent_comment</th>\n      <th>parent_comment_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n      <td>[yeah, i, get, that, argument, at, this, point...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n      <td>[the, blazer, and, maverick, the, west, 5, and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They're favored to win.</td>\n      <td>[theyre, favored, to, win]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>deadass don't kill my buzz</td>\n      <td>[deadass, dont, kill, my, buzz]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n      <td>[yep, can, confirm, i, saw, the, tool, they, u...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>do you find ariana grande sexy ?</td>\n      <td>[do, you, find, ariana, grande, sexy]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>What's your weird or unsettling Trick or Treat...</td>\n      <td>[whats, your, weird, or, unsettling, trick, or...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Probably Sephiroth. I refuse to taint his grea...</td>\n      <td>[probably, sephiroth, i, refuse, to, taint, hi...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>What to upgrade? I have $500 to spend (mainly ...</td>\n      <td>[what, to, upgrade, i, have, 500, to, spend, m...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Probably count Kanye out Since the rest of his...</td>\n      <td>[probably, count, kanye, out, since, the, rest...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"parent_comment\", \"parent_comment_tokens\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csO8vZCeB-V7",
    "outputId": "9d8ba787-2d6d-42a2-df1b-d662008ddc83",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1010773,)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parent_comment_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "c1KHKynAB-V8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = data[['comment_tokens', 'parent_comment_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "q1-UQkLdB-V8",
    "outputId": "3e109d85-0143-4ae0-d65b-b026629bd091",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                      comment_tokens  \\\n0                                      [nc, and, nh]   \n1  [you, do, know, west, team, play, against, wes...   \n2  [they, were, underdog, earlier, today, but, si...   \n3  [this, meme, isnt, funny, none, of, the, new, ...   \n4              [i, could, use, one, of, those, tool]   \n5  [i, dont, pay, attention, to, her, but, a, lon...   \n6  [trick, or, treating, in, general, is, just, w...   \n7                 [blade, masterymasamune, or, gtfo]   \n8  [you, dont, have, to, you, have, a, good, buil...   \n9          [i, would, love, to, see, him, at, lolla]   \n\n                               parent_comment_tokens  \n0  [yeah, i, get, that, argument, at, this, point...  \n1  [the, blazer, and, maverick, the, west, 5, and...  \n2                         [theyre, favored, to, win]  \n3                    [deadass, dont, kill, my, buzz]  \n4  [yep, can, confirm, i, saw, the, tool, they, u...  \n5              [do, you, find, ariana, grande, sexy]  \n6  [whats, your, weird, or, unsettling, trick, or...  \n7  [probably, sephiroth, i, refuse, to, taint, hi...  \n8  [what, to, upgrade, i, have, 500, to, spend, m...  \n9  [probably, count, kanye, out, since, the, rest...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_tokens</th>\n      <th>parent_comment_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[nc, and, nh]</td>\n      <td>[yeah, i, get, that, argument, at, this, point...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[you, do, know, west, team, play, against, wes...</td>\n      <td>[the, blazer, and, maverick, the, west, 5, and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[they, were, underdog, earlier, today, but, si...</td>\n      <td>[theyre, favored, to, win]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[this, meme, isnt, funny, none, of, the, new, ...</td>\n      <td>[deadass, dont, kill, my, buzz]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[i, could, use, one, of, those, tool]</td>\n      <td>[yep, can, confirm, i, saw, the, tool, they, u...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[i, dont, pay, attention, to, her, but, a, lon...</td>\n      <td>[do, you, find, ariana, grande, sexy]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[trick, or, treating, in, general, is, just, w...</td>\n      <td>[whats, your, weird, or, unsettling, trick, or...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[blade, masterymasamune, or, gtfo]</td>\n      <td>[probably, sephiroth, i, refuse, to, taint, hi...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[you, dont, have, to, you, have, a, good, buil...</td>\n      <td>[what, to, upgrade, i, have, 500, to, spend, m...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[i, would, love, to, see, him, at, lolla]</td>\n      <td>[probably, count, kanye, out, since, the, rest...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EodAkf1pB-V9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = data[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "NfBMg_DWB-V9",
    "outputId": "45930e2e-478f-4de7-b74a-32cbe6e5b0c7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         label\n1010816      1\n1010817      1\n1010818      1\n1010819      1\n1010820      1\n1010821      1\n1010822      1\n1010823      1\n1010824      1\n1010825      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1010816</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010817</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010818</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010819</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010820</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010821</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010822</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010823</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010824</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1010825</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Split data into different sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6mnu3l3QB-V-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, train_size= 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iCPPPTHrB-V-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "j3pn34veB-V-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "X_val = X_val.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9dWmr6L7B-V-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop = True)\n",
    "y_valid = y_val.reset_index(drop = True)\n",
    "y_test = y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "X_train_s = X_train.copy()\n",
    "X_val_s = X_val.copy()\n",
    "X_test_s = X_test.copy()\n",
    "y_train_s = y_train.copy()\n",
    "y_val_s = y_val.copy()\n",
    "y_test_s = y_test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "Ej_7dKWPB-V_",
    "outputId": "65e77730-308e-4c84-a9c8-eddb5a84f132",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                      comment_tokens  \\\n0    [i, have, read, it, and, it, doesnt, say, that]   \n1                                 [narcissist, much]   \n2  [apparently, people, are, getting, nightmare, ...   \n3  [i, will, die, a, happy, man, probably, around...   \n4  [but, kung, fu, will, help, you, to, defend, y...   \n5  [thats, cuz, the, u, let, craig, venter, run, ...   \n6                    [dont, steal, my, thunder, bro]   \n7                                  [what, the, fuck]   \n8                                              [lol]   \n9  [thus, seems, like, a, lot, of, work, for, som...   \n\n                               parent_comment_tokens  \n0  [the, first, amendment, protects, freedom, of,...  \n1  [personalized, bathrobe, hey, guy, im, looking...  \n2          [freak, out, ad, did, i, miss, something]  \n3  [thanks, for, your, insight, a, is, that, othe...  \n4  [a, an, asian, male, our, group, is, also, the...  \n5  [til, that, despite, early, concern, the, huma...  \n6  [the, fuck, are, you, talking, about, ram, hav...  \n7  [come, over, here, young, boy, and, grip, this...  \n8    [osama, bin, laden, gave, you, an, upvote, lol]  \n9             [the, clearest, image, of, pluto, yet]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_tokens</th>\n      <th>parent_comment_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[i, have, read, it, and, it, doesnt, say, that]</td>\n      <td>[the, first, amendment, protects, freedom, of,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[narcissist, much]</td>\n      <td>[personalized, bathrobe, hey, guy, im, looking...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[apparently, people, are, getting, nightmare, ...</td>\n      <td>[freak, out, ad, did, i, miss, something]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[i, will, die, a, happy, man, probably, around...</td>\n      <td>[thanks, for, your, insight, a, is, that, othe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[but, kung, fu, will, help, you, to, defend, y...</td>\n      <td>[a, an, asian, male, our, group, is, also, the...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[thats, cuz, the, u, let, craig, venter, run, ...</td>\n      <td>[til, that, despite, early, concern, the, huma...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[dont, steal, my, thunder, bro]</td>\n      <td>[the, fuck, are, you, talking, about, ram, hav...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[what, the, fuck]</td>\n      <td>[come, over, here, young, boy, and, grip, this...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[lol]</td>\n      <td>[osama, bin, laden, gave, you, an, upvote, lol]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[thus, seems, like, a, lot, of, work, for, som...</td>\n      <td>[the, clearest, image, of, pluto, yet]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-a5h-ocrB-V_",
    "outputId": "1810412d-9f3b-44eb-9724-9db86a65f358",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "606463"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qsArxBw3B-WA",
    "outputId": "a5bd4fdb-e21d-4b30-b2f4-683db909d6e8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1010773, 7)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Compute TFIDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GRyg5ZxWB-WA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Since comments are already preprocessed and tokenized,\n",
    "# the vectorizer only needs to take tokens as they are.\n",
    "tfidfVectorizer = TfidfVectorizer(analyzer=lambda tokens: tokens)\n",
    "corpus = pd.concat([X_train['comment_tokens'], X_train['parent_comment_tokens']])\n",
    "tfidfMatrix = tfidfVectorizer.fit_transform(corpus)\n",
    "vocabulary = tfidfVectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzqK3tVAB-WA",
    "outputId": "e61853e7-aa9e-4c38-add0-f4e2b1504bcd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1212926, 284355)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRILmYjTB-WB",
    "outputId": "4c441989-7674-4ddf-a61b-893986e8bbe8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<1212926x284355 sparse matrix of type '<class 'numpy.float64'>'\n\twith 17840005 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tfidf matrix has 2021546 rows,\n",
    "# where the 0 ~ 606462 rows represent original comments in the same order as those in dataset,\n",
    "# and the 606463 ~ 1212926 rows represent parent comments in the same order as those in dataset;\n",
    "# and the tfidf matrix has 395961 columns, each represents a word in the vocabulary.\n",
    "tfidfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uY-W2yFaB-WB",
    "outputId": "61235d3a-e6ac-4def-c4e0-ba07c438b02e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'i': 125746,\n 'would': 277802,\n 'not': 177790,\n 'have': 117879,\n 'known': 142057,\n 'if': 126709,\n 'he': 118302,\n 'wa': 270364,\n '18': 4823,\n 'you': 281918,\n 'didnt': 77385,\n 'tell': 248641,\n 'me': 159132,\n 'exactly': 91967,\n 'the': 250200,\n 'reason': 208134,\n 'why': 275041,\n 'area': 30935,\n 'damage': 71333,\n 'and': 27310,\n 'supposedly': 243985,\n 'life': 148434,\n 'on': 182046,\n 'hit': 121618,\n 'cause': 55056,\n 'lag': 144480,\n 'is': 132477,\n 'due': 83744,\n 'to': 253679,\n 'sheer': 226982,\n 'number': 178980,\n 'of': 180544,\n 'extra': 93239,\n 'computation': 63593,\n 'server': 225468,\n 'now': 178380,\n 'ha': 115046,\n 'perform': 190689,\n 'human': 124553,\n 'prison': 199714,\n 'etc': 90739,\n 'kulcha': 143550,\n 'forgot': 101022,\n 'add': 21606,\n 'this': 251375,\n 'legendary': 146870,\n 'kairoplays': 138122,\n 'they': 250974,\n 'are': 30934,\n 'geez': 107076,\n 'it': 133070,\n 'just': 137485,\n 'a': 19202,\n 'joke': 136399,\n 'that': 249955,\n 'black': 43575,\n 'people': 190341,\n 'ruin': 217748,\n 'everything': 91643,\n 'white': 274540,\n 'grow': 113315,\n 'some': 234501,\n 'ball': 37004,\n 'man': 155461,\n 'gag': 105084,\n 'indeed': 128870,\n 'sound': 235357,\n 'like': 148795,\n 'something': 234591,\n 'trump': 257737,\n 'say': 221066,\n 'found': 101399,\n 'leaver': 146458,\n 'ive': 133600,\n 'heard': 118606,\n 'feature': 96178,\n 'before': 40017,\n 'but': 50878,\n 'only': 182525,\n 'negative': 172492,\n 'privacy': 199742,\n 'invasion': 131657,\n 'talking': 246513,\n 'point': 195241,\n 'im': 127470,\n 'pming': 194914,\n 'my': 169992,\n 'paypal': 189145,\n 'address': 21683,\n 'please': 194445,\n 'send': 224850,\n '800': 16784,\n 'euro': 91099,\n 'so': 233627,\n 'can': 52877,\n 'buy': 51398,\n 'ticket': 252392,\n 'find': 97942,\n 'out': 184659,\n 'no': 175103,\n 'armada': 31330,\n 'westballz': 273725,\n 'many': 156259,\n 'brazilian': 47754,\n 'fan': 94886,\n 'there': 250738,\n 'bet': 41490,\n 'large': 145258,\n 'them': 250432,\n 'heritage': 119844,\n 'see': 223693,\n 'first': 98427,\n 'thing': 251131,\n 'should': 228557,\n 'tipped': 253190,\n 'off': 180577,\n 'batman': 38604,\n 'had': 115306,\n 'gun': 114190,\n 'isnt': 132824,\n 'smoking': 232868,\n 'metal': 161325,\n 'great': 112354,\n 'for': 100687,\n 'iphone': 132027,\n 'most': 167327,\n 'megapixels': 159996,\n 'more': 166915,\n 'than': 249853,\n 'your': 282091,\n 'dslr': 83359,\n 'tha': 249776,\n 'best': 41371,\n 'camera': 52667,\n 'ever': 91460,\n 'promise': 200738,\n 'stipend': 240208,\n 'doesnt': 80231,\n 'mean': 159191,\n 'expecting': 92732,\n 'compensation': 63283,\n 'low': 151791,\n 'effort': 86406,\n 'attempt': 33696,\n 'at': 33205,\n 'memeing': 160433,\n 'enjoy': 88978,\n 'obamas': 179710,\n 'climate': 60614,\n 'action': 21265,\n 'speech': 236303,\n 'labeled': 144175,\n 'wouldistraction': 278079,\n 'from': 102912,\n 'real': 207861,\n 'purpose': 202957,\n 'g20': 104807,\n 'how': 123909,\n 'much': 168492,\n 'coal': 61348,\n 'we': 272436,\n 'sell': 224500,\n 'rest': 212349,\n 'world': 277545,\n 'weak': 272449,\n 'cheat': 57202,\n 'even': 91397,\n 'show': 228642,\n 'which': 274363,\n 'way': 272226,\n 'theyre': 250995,\n 'looking': 151145,\n 'never': 173380,\n 'safe': 219175,\n 'fall': 94641,\n 'asleep': 32489,\n 'wheel': 274226,\n 'equality': 89754,\n 'been': 39912,\n 'achieved': 21002,\n 'yeah': 280790,\n 'kind': 140952,\n 'dick': 77165,\n 'breaking': 47848,\n 'ground': 113214,\n 'obviously': 180083,\n 'thats': 250061,\n 'enough': 89072,\n 'know': 141966,\n 'an': 27055,\n 'asshole': 32798,\n 'in': 128309,\n 'looked': 151129,\n 'mirror': 164019,\n 'noticed': 178042,\n 'yourself': 282125,\n 'defacing': 74026,\n 'sign': 229524,\n 'yoga': 281682,\n 'pant': 187475,\n 'make': 154851,\n 'up': 264135,\n 'as': 32069,\n 'used': 265072,\n 'be': 39279,\n 'flat': 99225,\n 'when': 274278,\n 'stretched': 241240,\n 'earth': 85141,\n 'got': 111361,\n 'rounded': 216788,\n 'lot': 151520,\n 'bigger': 42384,\n 'considering': 64638,\n 'money': 166075,\n 'likely': 148888,\n 'really': 207976,\n 'insult': 130716,\n 'matter': 157905,\n 'because': 39645,\n 'everyone': 91608,\n 'what': 274042,\n 'team': 247875,\n 'save': 220935,\n 'disappointment': 78462,\n 'october': 180313,\n 'guy': 114604,\n 'hazmat': 118138,\n 'going': 110465,\n 'get': 108011,\n '10': 907,\n 'lash': 145384,\n 'drying': 83288,\n 'weather': 272620,\n 'tsm': 258116,\n 'ufc190': 260319,\n 'bro': 48505,\n 'thanks': 249888,\n 'very': 267804,\n 'classify': 60197,\n 'skream': 231250,\n 'nero': 173016,\n 'indie': 129000,\n 'weird': 273146,\n 'these': 250897,\n 'sexual': 225933,\n 'desire': 76024,\n 'seems': 223784,\n 'mixed': 164730,\n 'with': 276317,\n 'outright': 185035,\n 'aggression': 23133,\n 'towards': 255275,\n 'woman': 276807,\n 'yes': 281319,\n 'totally': 255024,\n 'said': 219339,\n 'all': 25014,\n 'addict': 21636,\n 'filthy': 97859,\n 'homeless': 122616,\n 'boston': 46617,\n 'bomber': 45720,\n 'fleshlights': 99423,\n 'removing': 211080,\n 'pearl': 189548,\n 'completely': 63410,\n 'remove': 211054,\n 'skill': 230968,\n 'minecraft': 163402,\n 'pvp': 203270,\n 'think': 251216,\n 'aston': 33043,\n 'martin': 157087,\n 'vantage': 266502,\n 'gt3': 113586,\n 'chevrolet': 57677,\n 'sareni': 220522,\n 'camaro': 52612,\n 'callaway': 52439,\n 'corvette': 66334,\n 'cadillac': 52050,\n 'atsvr': 33611,\n 'bentley': 40913,\n 'continental': 65053,\n 'lexus': 147858,\n 'rcf': 207389,\n 'nissan': 174843,\n 'gtr': 113678,\n 'lamborghini': 144752,\n 'huracan': 124923,\n 'ferrari': 96995,\n '488gtb': 12732,\n 'mercedes': 160911,\n 'gt': 113582,\n 'dodge': 80148,\n 'viper': 268771,\n 'maserati': 157265,\n 'coupe': 66910,\n 'grand': 111961,\n 'sportif': 237260,\n 'acura': 21472,\n 'vaporware': 266573,\n 'nsx': 178737,\n 'update': 264217,\n 'mclaren': 158877,\n '650s': 15220,\n 'bmw': 45050,\n 'm6': 153430,\n 'rufporsche': 217713,\n '991': 18463,\n 'ford': 100789,\n 'date': 72194,\n 'wrong': 278631,\n 'were': 273621,\n 'telling': 248658,\n 'story': 240690,\n 'about': 20216,\n 'merlin': 161074,\n 'century': 55878,\n 'born': 46459,\n 'nice': 174038,\n 'try': 257977,\n 'cop': 65665,\n 'who': 274772,\n 'need': 172342,\n 'their': 250389,\n 'arrest': 31672,\n 'quota': 204654,\n 'fair': 94391,\n 'responding': 212286,\n 'someone': 234541,\n 'saying': 221106,\n 'prince': 199596,\n 'baby': 35938,\n 'boomer': 46105,\n 'idol': 126566,\n 'by': 51588,\n 'americ': 26435,\n 'dont': 80870,\n 'uncle': 261598,\n 'will': 275541,\n 'cosy': 66521,\n 'her': 119648,\n 'gmo': 109842,\n 'label': 144174,\n 'well': 273262,\n 'lol': 150653,\n 'trust': 257898,\n 'anything': 29552,\n 'lebron': 146518,\n 'practicing': 197886,\n 'writing': 278604,\n 'article': 31886,\n 'gameinformer': 105544,\n 'start': 238952,\n 'lettin': 147643,\n 'post': 196824,\n 'ratm': 206913,\n 'then': 250548,\n 'gonna': 110793,\n 'postin': 197017,\n 'icp': 126293,\n 'carryin': 54251,\n 'hatchet': 117668,\n 'everywhere': 91678,\n 'hammer': 116274,\n 'may': 158194,\n 'softer': 234079,\n 'industrial': 129163,\n 'diamond': 77031,\n 'le': 146165,\n 'g2': 104806,\n 'made': 153976,\n 'hell': 119262,\n 'sooner': 234910,\n 'others': 184452,\n 'his': 121500,\n 'case': 54425,\n 'maybe': 158211,\n 'somethings': 234629,\n 'changed': 56593,\n 'ii': 127003,\n 'diid': 77800,\n 'alonso': 25622,\n '2010': 6410,\n 'germany': 107876,\n 'gp': 111656,\n 'kinda': 140954,\n 'bf2': 41853,\n 'ringing': 214451,\n 'ear': 85034,\n 'however': 123936,\n 'radius': 205342,\n 'pretty': 199267,\n 'small': 232409,\n 'effect': 86340,\n 'triggered': 256954,\n 'dead': 72919,\n 'or': 183544,\n 'almost': 25541,\n 'explosion': 92989,\n 'ottawa': 184533,\n 'typical': 259713,\n 'where': 274302,\n 'god': 110127,\n 'name': 170848,\n 'boogie': 45991,\n 'damnit': 71440,\n 'fucking': 103518,\n 'bless': 44190,\n 'next': 173746,\n 'tupac': 258644,\n 'shakur': 226407,\n 'album': 24499,\n 'coming': 62765,\n 'am': 26104,\n 'shocked': 228129,\n 'appalled': 30056,\n 'surprising': 244237,\n 'revelation': 212931,\n 'increased': 128773,\n 'ult': 260885,\n 'nerfed': 172978,\n 'lucian': 152382,\n 'into': 131442,\n '110': 1919,\n 'crunchy': 68891,\n 'peanut': 189535,\n 'butter': 51172,\n 'want': 271233,\n 'epic': 89583,\n 'mount': 167661,\n 'play': 194100,\n 'falstad': 94738,\n 'inpectah': 130185,\n 'deck': 73635,\n 'old': 181509,\n 'dirty': 78361,\n 'stuff': 241662,\n 'pax': 189052,\n '2': 6115,\n 'vaporizer': 266566,\n 'sharpstone': 226743,\n 'grinder': 112940,\n 'bic': 42167,\n 'lighter': 148670,\n 'safety': 219224,\n 'raw': 207051,\n 'paper': 187620,\n 'visine': 268931,\n 'bent': 40901,\n 'plastic': 193976,\n 'card': 53715,\n 'stored': 240580,\n 'pelican': 189967,\n '1020': 1351,\n 'pic': 192420,\n 'right': 214104,\n 'wicket': 275141,\n 'croque': 68534,\n 'also': 25804,\n 'cricket': 68108,\n 'glad': 109298,\n 'fuck': 103414,\n 'banned': 37592,\n 'fonte': 100465,\n 'por': 196418,\n 'favor': 95872,\n 'taste': 247296,\n 'chalk': 56385,\n 'happens': 116861,\n 'same': 219817,\n 'factory': 94180,\n 'chocolate': 58430,\n 'bar': 37730,\n 'debug': 73402,\n 'command': 62800,\n 'look': 151113,\n 'progress': 200524,\n 'youve': 282239,\n 'paved': 188998,\n 'massive': 157387,\n 'reconstruction': 208778,\n 'project': 200596,\n 'mexico': 161697,\n 'probably': 199933,\n 'cursed': 70070,\n 'leadership': 146198,\n 'candidate': 53039,\n 'august': 34039,\n 'underground': 261933,\n 'anyone': 29517,\n 'eat': 85358,\n 'shit': 227703,\n 'mar': 156402,\n 'criminal': 68172,\n 'belong': 40588,\n 'jail': 134130,\n 'thought': 251695,\n 'smelled': 232661,\n 'bad': 36386,\n 'outside': 185070,\n 'remember': 210926,\n 'correctly': 66169,\n '200k': 6356,\n 'profit': 200382,\n '60k': 14845,\n 'ish': 132602,\n 'exp': 92676,\n 'hour': 123738,\n 'agree': 23294,\n 'atheist': 33325,\n 'definitely': 74239,\n 'worse': 277683,\n 'jew': 135506,\n 'nazigermany': 171801,\n 'least': 146407,\n 'reagan': 207838,\n 'fault': 95790,\n 'anymore': 29492,\n 'worry': 277672,\n 'sure': 244091,\n 'edited': 85983,\n 'bitch': 43205,\n 'oh': 181050,\n 'good': 110836,\n 'john': 136283,\n 'mccain': 158514,\n 'involved': 131844,\n 'go': 109959,\n 'swimmingly': 245031,\n 'funny': 104198,\n 'clicked': 60551,\n 'symbol': 245356,\n 'duality': 83475,\n 'flying': 100020,\n 'spaghetti': 235843,\n 'monster': 166420,\n 'come': 62628,\n 'u': 259846,\n 'software': 234104,\n 'developer': 76512,\n 'integrity': 130830,\n 'privilege': 199804,\n 'half': 115852,\n 'period': 190757,\n 'hasnt': 117587,\n 'scored': 222407,\n 'return': 212807,\n 'him': 121133,\n 'toronto': 254855,\n 'saw': 221013,\n 'similar': 229863,\n 'question': 204248,\n 'below': 40600,\n 'damn': 71423,\n 'slowed': 232198,\n 'down': 81723,\n 'time': 252756,\n 'wait': 270742,\n 'those': 251645,\n 'special': 236169,\n 'sun': 243136,\n 'power': 197512,\n 'capable': 53332,\n 'battery': 38687,\n 'v': 265773,\n 'ordinary': 183715,\n 'grid': 112793,\n 'one': 182161,\n 'karma': 138750,\n 'butcher': 51005,\n 'meloche': 160303,\n 'still': 240106,\n 'pipeline': 193216,\n 'usually': 265381,\n 'combat': 62533,\n 'focused': 100243,\n 'character': 56786,\n 'charisma': 56902,\n 'intelligence': 130860,\n 'use': 265060,\n 'sense': 224956,\n 'done': 80755,\n 'without': 276386,\n 'violence': 268729,\n 'care': 53818,\n 'put': 203127,\n 'bullet': 50118,\n 'strategic': 240972,\n 'approach': 30297,\n 'game': 105422,\n 'super': 243353,\n 'easy': 85313,\n 'showed': 228670,\n 'focus': 100237,\n 'helicopter': 119228,\n 'parenting': 187947,\n 'future': 104496,\n 'free': 102095,\n 'virus': 268864,\n 'steal': 239412,\n 'steam': 239453,\n 'console': 64684,\n 'father': 95654,\n 'year': 280937,\n 'bigt': 42480,\n 'stock': 240286,\n 'youtube': 282199,\n 'choice': 58456,\n 'live': 149798,\n 'society': 233854,\n 'do': 79985,\n 'under': 261816,\n 'public': 202272,\n 'scrutiny': 222883,\n 'international': 131149,\n 'hall': 116086,\n 'pain': 186835,\n 'murdered': 169303,\n 'other': 184426,\n 'sport': 237242,\n 'successful': 242527,\n 'star': 238759,\n 'dedicated': 73848,\n 'winning': 276006,\n 'assholeish': 32808,\n 'arrogant': 31727,\n 'goddamn': 110169,\n 'commie': 62931,\n 'scandaniavian': 221388,\n 'leftist': 146732,\n 'league': 146266,\n 'giving': 109220,\n 'medal': 159498,\n 'enforcing': 88777,\n 'stopped': 240530,\n 'assault': 32668,\n 'murder': 169295,\n 'happening': 116844,\n 'gaggle': 105104,\n 'cattle': 54988,\n 'little': 149729,\n 'girl': 109018,\n 'saved': 220940,\n 'becoming': 39730,\n 'murderer': 169306,\n '11': 1918,\n 'instead': 130604,\n 'mother': 167431,\n 'flair': 98977,\n 'read': 207738,\n 'wrote': 278689,\n 'confirm': 64120,\n 'douchebaggery': 81577,\n 'fucked': 103462,\n 'hardworking': 117204,\n 'trying': 257999,\n 'living': 149906,\n 'honestly': 122862,\n 'ref': 209610,\n 'playing': 194284,\n 'arent': 31029,\n 'grandfather': 111986,\n 'dated': 72201,\n 'democrat': 75020,\n 'body': 45308,\n 'shut': 229034,\n 'until': 263917,\n 'nmbs': 175029,\n 'sabena': 218912,\n 'ti': 252313,\n 'night': 174360,\n 'christmas': 58791,\n 'last': 145419,\n '12': 2416,\n 'month': 166523,\n '2013': 6460,\n 'weve': 273878,\n 'told': 254060,\n 'hurry': 124986,\n 'wont': 277020,\n 'release': 210619,\n 'convenient': 65319,\n 'patriarchy': 188793,\n 'coincides': 61916,\n 'spread': 237428,\n 'zika': 283594,\n 'catnip': 54947,\n 'cat': 54707,\n 'such': 242587,\n 'ethical': 90844,\n 'objective': 179850,\n 'review': 213028,\n 'unfair': 262393,\n 'liverpool': 149853,\n 'sheffield': 227001,\n 'yea': 280745,\n 'europe': 91153,\n 'teach': 247826,\n 'defense': 74123,\n 'kindergarten': 140977,\n 'ya': 280254,\n 'college': 62194,\n 'terrible': 249234,\n 'hmm': 121914,\n 'alright': 25763,\n 'golf': 110672,\n 'gentleman': 107606,\n 'lady': 144408,\n 'forbidden': 100711,\n 'here': 119714,\n 'hunter': 124857,\n 'peasant': 189569,\n 'social': 233768,\n 'justice': 137541,\n 'long': 150922,\n 'benefit': 40743,\n 'male': 155082,\n 'redditors': 209073,\n 'tagged': 246151,\n 'overzealous': 185995,\n 'judgemental': 137089,\n 'keyboard': 140017,\n 'warrior': 271600,\n 'youre': 282096,\n 'big': 42313,\n 'strong': 241403,\n 'beat': 39480,\n 'must': 169664,\n 'deserved': 75932,\n 'eephus': 86272,\n 'plate': 194007,\n 'pc': 189246,\n 'judge': 137081,\n 'somebody': 234508,\n 'ignore': 126918,\n 'fanboy': 94914,\n 'regardless': 209926,\n 'mighty': 162681,\n 'river': 214819,\n 'ceo': 55888,\n 'indicated': 128984,\n '64000': 15108,\n 'vehicle': 267075,\n 'charging': 56887,\n 'overnight': 185640,\n '20': 6116,\n 'new': 173462,\n 'geothermal': 107774,\n 'plant': 193922,\n 'she': 226874,\n 'listen': 149574,\n 'mika': 162750,\n 'tv': 258978,\n 'morning': 167090,\n 'clinton': 60679,\n 'average': 34885,\n 'american': 26461,\n 'did': 77320,\n 'lizard': 149931,\n 'cost': 66428,\n 'dog': 80266,\n 'healthy': 118580,\n 'teeth': 248400,\n 'pointy': 195315,\n '4th': 13138,\n 'higher': 120634,\n 'depends': 75477,\n 'bought': 46816,\n 'headline': 118400,\n 'newey': 173518,\n 'unwilling': 264063,\n 'tweak': 259095,\n 'design': 75973,\n 'ricciardo': 213724,\n 'danica': 71606,\n 'patrick': 188814,\n 'could': 66641,\n '50lbs': 13546,\n 'heavier': 118799,\n 'too': 254373,\n 'system': 245604,\n 'force': 100726,\n 'addicted': 21637,\n 'crack': 67381,\n 'tough': 255155,\n 'problem': 199983,\n 'phoenix': 192053,\n 'clan': 60004,\n 'violet': 268758,\n 'flame': 99031,\n 'blood': 44506,\n 'heir': 119140,\n 'any': 29455,\n 'saint': 219393,\n 'altough': 25970,\n 'decade': 73422,\n 'passed': 188431,\n 'gotten': 111446,\n 'couple': 66913,\n 'kid': 140463,\n 'subscribed': 242292,\n 'threadsall': 251839,\n 'subreddits': 242274,\n 'highest': 120662,\n 'upvotes': 264645,\n 'back': 36073,\n 'psypokes': 202154,\n 'slowducks': 232197,\n 'work': 277381,\n 'leftright': 146759,\n 'picture': 192585,\n 'taken': 246367,\n 'brightnessgamma': 48293,\n 'person': 191093,\n 'left': 146698,\n 'dayz': 72579,\n 'videogame': 268254,\n 'colour': 62434,\n 'screen': 222637,\n 'cheap': 57155,\n 'club': 61046,\n 'displacing': 79108,\n 'czech': 70685,\n 'core': 65904,\n 'petr': 191439,\n 'tomas': 254137,\n 'bubble': 49486,\n 'machine': 153726,\n 'academic': 20571,\n 'result': 212483,\n 'accurate': 20865,\n 'measure': 159319,\n 'love': 151670,\n 'drop': 82943,\n 'second': 223427,\n 'after': 22786,\n 'doe': 80196,\n 'animation': 27824,\n 'men': 160563,\n 'advise': 22253,\n 'orientation': 183883,\n 'day': 72412,\n 'piercing': 192700,\n 'different': 77565,\n 'side': 229230,\n 'straight': 240830,\n 'gay': 106603,\n 'always': 26062,\n 'wanted': 271238,\n 'micro': 162064,\n 'van': 266368,\n 'city': 59741,\n 'every': 91563,\n 'single': 230113,\n 'fat': 95593,\n 'decendents': 73513,\n 'african': 22717,\n 'slave': 231698,\n 'came': 52647,\n 'country': 66847,\n 'f35': 93708,\n 'mission': 164435,\n 'poor': 196168,\n 'airlifter': 23926,\n 'actually': 21418,\n 'clever': 60494,\n 'testing': 249444,\n 'hadnt': 115357,\n 'litterally': 149713,\n 'unplayable': 263360,\n 'law': 145833,\n 'intent': 130917,\n 'thread': 251818,\n 'rabbit': 204948,\n 'rooster': 216375,\n 'bombed': 45718,\n 'legit': 146957,\n 'believe': 40435,\n 'wish': 276236,\n 'place': 193691,\n 'variant': 266626,\n 'within': 276365,\n 'resource': 212207,\n 'pack': 186535,\n 'having': 117946,\n 'random': 206199,\n 'sybian': 245272,\n 'modeled': 165401,\n 'elongs': 87620,\n 'wand': 271151,\n 'p2w': 186336,\n 'whale': 274010,\n 'fancy': 94942,\n 'skin': 231071,\n 'screw': 222705,\n 'imsa': 128273,\n 'uploads': 264396,\n 'race': 204996,\n 'week': 272902,\n 'inept': 129247,\n 'learn': 146360,\n 'background': 36157,\n 'process': 200091,\n 'running': 218004,\n 'crappy': 67589,\n '3rd': 11354,\n 'party': 188307,\n 'program': 200475,\n 'cover': 67056,\n 'snowball': 233469,\n 'through': 252022,\n 'minion': 163643,\n 'give': 109188,\n 'enemy': 88692,\n 'advantage': 22165,\n 'squishies': 237930,\n 'longer': 150958,\n 'hide': 120481,\n 'behind': 40185,\n 'german': 107844,\n 'mcdonalds': 158662,\n 'order': 183680,\n 'french': 102450,\n 'fry': 103203,\n 'march': 156477,\n 'take': 246351,\n 'pittance': 193493,\n 'doing': 80437,\n 'nothing': 178001,\n 'tote': 255048,\n 'rewarding': 213158,\n 'net': 173105,\n 'worth': 277733,\n 'pasf': 188384,\n '5': 13233,\n 'digit': 77725,\n 'financial': 97915,\n 'mobility': 165277,\n 'whore': 274942,\n 'somewhere': 234686,\n 'shia': 227261,\n 'kuffar': 143501,\n 'unsubbbed': 263792,\n 'quick': 204354,\n 'young': 282026,\n 'pretending': 199214,\n 'teenage': 248351,\n 'refugee': 209847,\n 'uk': 260729,\n 'scientist': 222218,\n 'lawyer': 145921,\n 'obvious': 180074,\n 'popular': 196358,\n 'anthony': 28322,\n 'fantano': 95076,\n 'guilty': 114038,\n 'being': 40244,\n 'hilarious': 120967,\n 'dude': 83661,\n 'helping': 119433,\n 'mortal': 167189,\n 'ensuring': 89172,\n 'soul': 235285,\n 'heaven': 118787,\n 'raunchy': 206972,\n 'group': 113255,\n 'campaign': 52761,\n 'except': 92128,\n '75': 16225,\n 'per': 190533,\n 'kill': 140694,\n 'dom': 80587,\n 'around': 31588,\n 'block': 44398,\n 'blue': 44763,\n 'jay': 134714,\n 'sub': 241972,\n 'tame': 246634,\n 'author': 34286,\n 'lived': 149811,\n 'mid': 162258,\n '1800s': 4859,\n 'suck': 242600,\n 'strategy': 240984,\n 'alliance': 25216,\n 'target': 247096,\n 'sway': 244804,\n 'vote': 269788,\n 'budweiser': 49716,\n 'uh': 260466,\n 'um': 261063,\n 'wow': 278268,\n 'reach': 207688,\n 'making': 154938,\n 'formerly': 101113,\n 'dominant': 80636,\n 'struggle': 241492,\n ...}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By using the following dict, we can obtain the index of a given word for querying tfidf matrix.\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHIXaRG8B-WC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we have learned word vectors, to compute the sentence vector of a comment,\n",
    "first fetch the corresponding row from the tfidf matrix (if it is the i-th original comment, then\n",
    "it is the i-th row; if it is the i-th parent comment, then it is the (1010773 + i)-th row),\n",
    "second for each word in the comment find its corresponding tfidf value from the row,\n",
    "third compute the average of the word vectors weighted by corresponding tfidf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWkN9_eTB-WC",
    "outputId": "d5cf9334-8fdb-4305-f60d-f72b77456c4c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1212926, 284355)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eL8lq1BGB-WC",
    "outputId": "f9a984cb-9e56-4274-e86c-0eaf3418b3e3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfMatrix[0,249920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwNKPUx5B-WC",
    "outputId": "d78f7e9d-0f6f-44c3-beb9-516ec01e0281",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "250061"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['thats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "VOWPXq_HB-WD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidfMatrixtest = tfidfVectorizer.transform(pd.concat([X_test['comment_tokens'], X_test['parent_comment_tokens']]))\n",
    "tfidfMatrixvalid = tfidfVectorizer.transform(pd.concat([X_val['comment_tokens'], X_val['parent_comment_tokens']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l5WiMpEB-WD",
    "outputId": "6e484943-6e6e-4ea7-cdae-2a08ca6cca74",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(404310, 284355)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfMatrixtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "bB1Eh7b8B-WE",
    "outputId": "1f354084-2bc7-4b3d-e6fe-48a5de7e33c3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                      comment_tokens  \\\n0                                       [foundation]   \n1                                     [soapy, water]   \n2  [to, be, fair, though, faze, do, still, have, ...   \n3                                         [oh, fuck]   \n4    [oh, the, joy, of, being, a, christian, nation]   \n5       [keep, them, donation, coming, he, can, win]   \n6                       [itt, unnecessary, politics]   \n7  [i, needed, a, trigger, warning, im, having, t...   \n8  [they, are, doing, awfully, well, for, the, bu...   \n9                              [you, are, a, genius]   \n\n                               parent_comment_tokens  \n0  [awesome, restaurant, on, main, st, downtowner...  \n1  [you, can, spray, it, with, water, and, look, ...  \n2  [nv, going, to, hold, onto, this, w, like, faz...  \n3           [michy, batshuayi, goal, v, watford, 11]  \n4  [even, more, so, american, want, revenge, amp,...  \n5  [sander, outpaces, clinton, in, fundraising, f...  \n6                                   [mean, spirited]  \n7  [i, like, you, the, right, hand, wa, lost, sav...  \n8  [logic, and, common, sense, do, not, prevail, ...  \n9  [yeah, the, reality, version, go, like, this, ...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_tokens</th>\n      <th>parent_comment_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[foundation]</td>\n      <td>[awesome, restaurant, on, main, st, downtowner...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[soapy, water]</td>\n      <td>[you, can, spray, it, with, water, and, look, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[to, be, fair, though, faze, do, still, have, ...</td>\n      <td>[nv, going, to, hold, onto, this, w, like, faz...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[oh, fuck]</td>\n      <td>[michy, batshuayi, goal, v, watford, 11]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[oh, the, joy, of, being, a, christian, nation]</td>\n      <td>[even, more, so, american, want, revenge, amp,...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[keep, them, donation, coming, he, can, win]</td>\n      <td>[sander, outpaces, clinton, in, fundraising, f...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[itt, unnecessary, politics]</td>\n      <td>[mean, spirited]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[i, needed, a, trigger, warning, im, having, t...</td>\n      <td>[i, like, you, the, right, hand, wa, lost, sav...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[they, are, doing, awfully, well, for, the, bu...</td>\n      <td>[logic, and, common, sense, do, not, prevail, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[you, are, a, genius]</td>\n      <td>[yeah, the, reality, version, go, like, this, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Apu6E71eB-WE",
    "outputId": "c124d6ac-8add-40a5-b01b-12b065f2af62",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "100687"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['for']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8A7jdafB-WE",
    "outputId": "d4bd6644-9df2-43ec-e534-cbe013df9418",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfMatrixtest[0, 100573]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Compute sentence vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "muyep1SGB-WF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(X, tfidfmat):\n",
    "    comment_featurevec = []\n",
    "    parent_featurevec = []\n",
    "    \n",
    "    for index, sample in X.iterrows():\n",
    "        \n",
    "        comment_vec = np.zeros(300)\n",
    "        parent_comment_vec = np.zeros(300)\n",
    "        comment_tokens = sample['comment_tokens']\n",
    "        parent_comment_tokens = sample['parent_comment_tokens']\n",
    "        counter1 = 0\n",
    "        \n",
    "        for token in comment_tokens:\n",
    "            \n",
    "            if token in glove and token in vocabulary:\n",
    "                counter1 += 1\n",
    "                comment_vec += tfidfmat[index, vocabulary[token]] * glove[token]\n",
    "        \n",
    "        counter2 = 0\n",
    "        for token in parent_comment_tokens:\n",
    "            \n",
    "            if token in glove and token in vocabulary:\n",
    "                counter2 += 1 \n",
    "                parent_comment_vec += tfidfmat[index+len(X), vocabulary[token]] * glove[token]\n",
    "        \n",
    "        if counter1 != 0:\n",
    "            comment_featurevec.append(comment_vec/counter1)\n",
    "        else:\n",
    "            comment_featurevec.append(comment_vec)\n",
    "        \n",
    "        if counter2 != 0:\n",
    "            parent_featurevec.append(parent_comment_vec/counter2)\n",
    "        else:\n",
    "            parent_featurevec.append(parent_comment_vec)\n",
    "            \n",
    "        \n",
    "    #print(len(comment_featurevec))\n",
    "    X['comment_weighted_vec'] = comment_featurevec\n",
    "    X['parent_weighted_vec'] = parent_featurevec\n",
    "    new_df = pd.DataFrame()\n",
    "        \n",
    "    return X\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wQ6QfXMtB-WF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = generate_dataset(X_train, tfidfMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "gnGtChVSB-WG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_val = generate_dataset(X_val, tfidfMatrixvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "WXFNbr7JB-WG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = generate_dataset(X_test, tfidfMatrixtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "mLRcW6yCB-WG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train[['comment_weighted_vec', 'parent_weighted_vec']]\n",
    "X_val = X_val[['comment_weighted_vec', 'parent_weighted_vec']]\n",
    "X_test = X_test[['comment_weighted_vec', 'parent_weighted_vec']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "kDk4t9dDB-WG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def featurize(dataset):\n",
    "    x = pd.DataFrame(dataset['parent_weighted_vec'].tolist(), columns = [i for i in range(300)])\n",
    "    y = pd.DataFrame(dataset['comment_weighted_vec'].tolist(), columns = [i for i in range(300, 600)])\n",
    "    return pd.concat([x, y], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Y2lcwA11B-WH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = featurize(X_train)\n",
    "X_val = featurize(X_val)\n",
    "X_test = featurize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Compute Glove sequence for LSTM (deleted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTDHL0Q9B-WM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Logistic Regression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 500).fit(X_train, y_train.values.ravel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5957408918898864"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5952066483638792"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test.iloc[2].values.reshape(1,-1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Feed Forward Neural Net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "device = get_default_device()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "x_tr = torch.tensor(X_train.iloc[:,0:600].values, dtype= torch.float32)\n",
    "x_val = torch.tensor(X_val.iloc[:,0:600].values, dtype= torch.float32)\n",
    "x_test = torch.tensor(X_test.iloc[:,0:600].values,dtype= torch.float32)\n",
    "y_tr = torch.tensor(list(y_train['label']), dtype= torch.long)\n",
    "y_vali = torch.tensor(list(y_val['label']), dtype= torch.long)\n",
    "y_tst = torch.tensor(list(y_test['label']), dtype= torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0811, -0.0394, -0.0260,  ..., -0.1191, -0.0266,  0.0586],\n        [-0.0340,  0.0345,  0.0137,  ..., -0.0060, -0.0006, -0.0163],\n        [-0.0231,  0.0317, -0.0088,  ..., -0.0011, -0.0802,  0.1426],\n        ...,\n        [ 0.3366,  0.5286, -0.2200,  ..., -0.0442,  0.0252,  0.1101],\n        [-0.0176,  0.0424, -0.0470,  ...,  0.1297,  0.2087, -0.0245],\n        [-0.0102,  0.0293,  0.0221,  ..., -0.1184,  0.0442,  0.0703]])"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 0, 1,  ..., 1, 1, 1])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.to(device)\n",
    "x_test.to(device)\n",
    "y_tr.to(device)\n",
    "y_vali.to(device)\n",
    "y_tst.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "class XY(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "  def __getitem__(self, i):\n",
    "    return self.x[i], self.y[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "trainset = XY(x_tr, y_tr)\n",
    "valset = XY(x_val, y_vali)\n",
    "testset = XY(x_test, y_tst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "input_dim = 600\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "output_dim = 2\n",
    "num_epochs = 100\n",
    "batch_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "SyrDlWZsB-WN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, layer_num, input_dim, hidden_dim, output_dim):\n",
    "      super(FFNN, self).__init__()\n",
    "      self.layers = []\n",
    "      for i in range(layer_num):\n",
    "        self.layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
    "        self.layers.append(nn.ReLU())\n",
    "      self.layers.append(nn.Linear(input_dim if layer_num == 0 else hidden_dim, output_dim))\n",
    "\n",
    "      self.layers = nn.ModuleList(self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "          x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "XTDqgpLnB-WN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = FFNN(num_layers, input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_q5BW3JB-WN",
    "outputId": "4fb8fe19-2e78-4a9e-e0fd-051f68a4c9fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch completed: 5\n",
      "\tTraining Loss: 0.567\n",
      "\tValidation Loss: 0.617\n",
      "\n",
      "Epoch completed: 10\n",
      "\tTraining Loss: 0.512\n",
      "\tValidation Loss: 0.664\n",
      "\n",
      "Epoch completed: 15\n",
      "\tTraining Loss: 0.467\n",
      "\tValidation Loss: 0.738\n",
      "\n",
      "Epoch completed: 20\n",
      "\tTraining Loss: 0.432\n",
      "\tValidation Loss: 0.834\n",
      "\n",
      "Epoch completed: 25\n",
      "\tTraining Loss: 0.405\n",
      "\tValidation Loss: 0.925\n",
      "\n",
      "Epoch completed: 30\n",
      "\tTraining Loss: 0.382\n",
      "\tValidation Loss: 1.022\n",
      "\n",
      "Epoch completed: 35\n",
      "\tTraining Loss: 0.364\n",
      "\tValidation Loss: 1.122\n",
      "\n",
      "Epoch completed: 40\n",
      "\tTraining Loss: 0.349\n",
      "\tValidation Loss: 1.224\n",
      "\n",
      "Epoch completed: 45\n",
      "\tTraining Loss: 0.335\n",
      "\tValidation Loss: 1.307\n",
      "\n",
      "Epoch completed: 50\n",
      "\tTraining Loss: 0.324\n",
      "\tValidation Loss: 1.399\n",
      "\n",
      "Epoch completed: 55\n",
      "\tTraining Loss: 0.313\n",
      "\tValidation Loss: 1.478\n",
      "\n",
      "Epoch completed: 60\n",
      "\tTraining Loss: 0.304\n",
      "\tValidation Loss: 1.567\n",
      "\n",
      "Epoch completed: 65\n",
      "\tTraining Loss: 0.296\n",
      "\tValidation Loss: 1.634\n",
      "\n",
      "Epoch completed: 70\n",
      "\tTraining Loss: 0.288\n",
      "\tValidation Loss: 1.724\n",
      "\n",
      "Epoch completed: 75\n",
      "\tTraining Loss: 0.281\n",
      "\tValidation Loss: 1.790\n",
      "\n",
      "Epoch completed: 80\n",
      "\tTraining Loss: 0.275\n",
      "\tValidation Loss: 1.876\n",
      "\n",
      "Epoch completed: 85\n",
      "\tTraining Loss: 0.268\n",
      "\tValidation Loss: 1.957\n",
      "\n",
      "Epoch completed: 90\n",
      "\tTraining Loss: 0.263\n",
      "\tValidation Loss: 2.058\n",
      "\n",
      "Epoch completed: 95\n",
      "\tTraining Loss: 0.258\n",
      "\tValidation Loss: 2.141\n",
      "\n",
      "Epoch completed: 100\n",
      "\tTraining Loss: 0.253\n",
      "\tValidation Loss: 2.184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_loss_min = math.inf\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for data, target in trainloader:\n",
    "      data = data.to(device)\n",
    "      target = target.to(device)\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)  \n",
    "      train_loss += loss.item()\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    for data, target in valloader:\n",
    "      data = data.to(device)\n",
    "      target = target.to(device)\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)\n",
    "      val_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(train_loss/len(trainloader))\n",
    "    val_losses.append(val_loss/len(valloader))\n",
    "\n",
    "    if val_loss <= val_loss_min:\n",
    "      torch.save(model.state_dict(), 'model.pt')\n",
    "      val_loss_min = val_loss\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      print(\"Epoch completed: \" + str(epoch+1) +\n",
    "            f\"\\n\\tTraining Loss: {train_losses[epoch]:.3f}\" + \n",
    "            f\"\\n\\tValidation Loss: {val_losses[epoch]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "tx1TuafAB-WP",
    "outputId": "a7d12a7d-c43c-4e02-ae7d-3d0839d9d898",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0LUlEQVR4nO3deXxU9bnH8c+TmewJISQEkAAB2RHZIlRBBbfiUlyqVVxRr2hb69JWq7ZVa9t7e1vrba0rKlo3sC4oUgQVFxSrEjZlByFA2BMg+zbJc/84QxhgEhLI5CQzz/v1yiszv3POzHNCmG9+Z/n9RFUxxhhjDhXldgHGGGNaJwsIY4wxQVlAGGOMCcoCwhhjTFAWEMYYY4KygDDGGBOUN1QvLCLdgBeBzkAtMEVV/37IOlcBv/I/LQF+rKrL/MtygWKgBvCpavaR3jM9PV2zsrKaaxeMMSbsLVq0KF9VOwZbFrKAAHzAL1R1sYgkA4tE5ANVXRmwzkbgdFXdKyLnAlOAUQHLx6lqfmPfMCsri5ycnGYp3hhjIoGIbKpvWcgCQlW3A9v9j4tFZBXQFVgZsM4XAZt8CWSGqh5jjDFN0yLnIEQkCxgGfNXAajcC7wU8V+B9EVkkIpMbeO3JIpIjIjm7d+9ulnqNMcaE9hATACKSBLwJ3KGqRfWsMw4nIMYENI9W1W0ikgF8ICKrVXX+oduq6hScQ1NkZ2fbuCHGGNNMQhoQIhKNEw6vqOpb9axzIvAscK6qFuxvV9Vt/u+7RGQGMBI4LCCOpLq6mry8PCoqKo5mF8wh4uLiyMzMJDo62u1SjDEhFsqrmAR4Dlilqo/Us0534C3gGlVdG9CeCET5z10kAucADx1NHXl5eSQnJ5OVlYVTkjlaqkpBQQF5eXn07NnT7XKMMSEWyh7EaOAa4FsRWepvuw/oDqCqTwH3A2nAE/4P7/2Xs3YCZvjbvMCrqjrnaIqoqKiwcGgmIkJaWhp2rseYyBDKq5g+Bxr8VFbV/wL+K0j7BmBIc9Vi4dB87GdpTOSwO6mNMaatUoXvPobP/y8kL28BEUIFBQUMHTqUoUOH0rlzZ7p27Vr3vKqqqsFtc3JyuO222474HqecckpzlWuMaUs2fwX//AG8dBHkPA/V5c3+FiG/zDWSpaWlsXTpUgAefPBBkpKS+OUvf1m33Ofz4fUG/yfIzs4mO/uIo4vwxRdfHHEdY0wY8FXB5v/Axk+dXsO2xZDYEcb/L2RfD97YZn9LC4gWNmnSJDp06MCSJUsYPnw4l19+OXfccQfl5eXEx8fz/PPP069fPz755BMefvhhZs2axYMPPsjmzZvZsGEDmzdv5o477qjrXSQlJVFSUsInn3zCgw8+SHp6OsuXL2fEiBG8/PLLiAizZ8/m5z//Oenp6QwfPpwNGzYwa9Ysl38SxphGKy2AFyfAzuUgHsjMhrN/DyfdCDGJIXvbiAqI3727gpXbgt6rd9QGHteOB34wqEnbrF27lg8//BCPx0NRURHz58/H6/Xy4Ycfct999/Hmm28ets3q1av5+OOPKS4upl+/fvz4xz8+7F6EJUuWsGLFCo477jhGjx7NggULyM7O5uabb2b+/Pn07NmTiRMnHtP+GmNCoNp/n1Z03OHLyvbAixdCwXq4eAr0Oxfi2rVIWREVEK3FZZddhsfjAaCwsJDrrruOdevWISJUV1cH3eb8888nNjaW2NhYMjIy2LlzJ5mZBw9dNXLkyLq2oUOHkpubS1JSEr169aq7b2HixIlMmTIlhHtnjGmSrYvhpYuhsghSe0LH/tB5MHQdDmm94fVJkL8WJk6D3me2aGkRFRBN/Us/VBITD3QJf/vb3zJu3DhmzJhBbm4uY8eODbpNbOyB44sejwefz9eodVRt9BFjWq0dy51wiGsHI2+C3Wtg92pYMxtnODrAEwNXtHw4QIQFRGtUWFhI165dAXjhhRea/fX79+/Phg0byM3NJSsri9dee63Z38MYcxR2r3EOHUUnwHXvQmrWgWWVJbB9mfOVeRJ0O8mVEu0yV5fdfffd3HvvvYwePZqamppmf/34+HieeOIJxo8fz5gxY+jUqRMpKSnN/j7GmCbYf15Bog4PB4DYJMgaDSf/xLVwAJBwOgSRnZ2th04YtGrVKgYMGOBSRa1DSUkJSUlJqCo//elP6dOnD3feeedRv579TI05RjNugW9fh5s+gi7NNmjEURGRRfXN2Gk9iAjwzDPPMHToUAYNGkRhYSE333yz2yUZE7nWfwjLpsGYO10PhyOxcxAR4M477zymHoMxpplUlsC7d0J6XzjtLrerOSILCGOMCZXyfbD0Fecu5+TjnKuTCrfADXNCcudzc7OAMMaYY7VnA3zxGPT9Phx/Jni8sHYuvHs7FG8/eN2Rk6H799yps4ksIIwx5liU7YGXL4U930HOc05PodMgWP8BZAyEy1+BlK5QtNUZMqPX6W5X3GgWEMYYc7R8lfDa1VCYB5P+7YTFkpdg85fOOYbT7jpwKCm5s7u1HgW7iinExo4dy9y5cw9q+9vf/sZPfvKTetfff6nueeedx759+w5b58EHH+Thhx9u8H3ffvttVq5cWff8/vvv58MPP2xi9caYeqnCzNtg0wK46AnIGgMDJ8BVr8O9W+CM37SJ8wwNCVlAiEg3EflYRFaJyAoRuT3IOiIij4rIehH5RkSGBywbLyJr/MvuCVWdoTZx4kSmT59+UNv06dMbNWje7Nmzad++/VG976EB8dBDD3HWWWcd1WsZYw6hCnN/Dd9Mh3G/hsGXul1RSISyB+EDfqGqA4DvAT8VkYGHrHMu0Mf/NRl4EkBEPMDj/uUDgYlBtm0TLr30UmbNmkVlZSUAubm5bNu2jVdffZXs7GwGDRrEAw88EHTbrKws8vPzAfjjH/9Iv379OOuss1izZk3dOs888wwnnXQSQ4YM4Yc//CFlZWV88cUXzJw5k7vuuouhQ4fy3XffMWnSJN544w0A5s2bx7Bhwxg8eDA33HBDXW1ZWVk88MADDB8+nMGDB7N69epQ/miMaZtqa2Dmz+DLx2HULW3ictWjFco5qbcD2/2Pi0VkFdAVWBmw2oXAi+rczv2liLQXkS5AFrDePzc1IjLdv27gtk333j2w49tjeonDdB4M5/6p3sVpaWmMHDmSOXPmcOGFFzJ9+nQuv/xy7r33Xjp06EBNTQ1nnnkm33zzDSeeeGLQ11i0aBHTp09nyZIl+Hw+hg8fzogRIwC45JJLuOmmmwD4zW9+w3PPPcfPfvYzJkyYwAUXXMCllx78l01FRQWTJk1i3rx59O3bl2uvvZYnn3ySO+64A4D09HQWL17ME088wcMPP8yzzz7bDD8kY8JAbQ2U7oY598CKGXDa3TDuPgjjedpb5ByEiGQBw4CvDlnUFdgS8DzP31Zfe7DXniwiOSKSs3v37maruTkFHmbaf3jpX//6F8OHD2fYsGGsWLHioMNBh/rss8+4+OKLSUhIoF27dkyYMKFu2fLlyzn11FMZPHgwr7zyCitWrGiwljVr1tCzZ0/69u0LwHXXXcf8+fPrll9yySUAjBgxgtzc3KPdZWPCx5dPwSMD4fcd4a/9nHA45w9wxq/DOhygBa5iEpEk4E3gDlU9dLaeYD9dbaD98EbVKcAUcMZiarCYBv7SD6WLLrqIn//85yxevJjy8nJSU1N5+OGHWbhwIampqUyaNImKiooGX0Pq+UWcNGkSb7/9NkOGDOGFF17gk08+afB1jjT21v4hw+sbUtyYiLLwWZjzK8g6FYZMdK5E6nwidB/ldmUtIqQ9CBGJxgmHV1T1rSCr5AHdAp5nAtsaaG+TkpKSGDt2LDfccAMTJ06kqKiIxMREUlJS2LlzJ++9916D25922mnMmDGD8vJyiouLeffdd+uWFRcX06VLF6qrq3nllVfq2pOTkykuLj7stfr3709ubi7r168H4KWXXuL009vOddnGtJhlr8G/fwn9zoNrZsCZv3XmbIiQcIAQ9iDE+ZP3OWCVqj5Sz2ozgVv95xhGAYWqul1EdgN9RKQnsBW4ArgyVLW2hIkTJ3LJJZcwffp0+vfvz7Bhwxg0aBC9evVi9OjRDW67f+7qoUOH0qNHD0499dS6Zb///e8ZNWoUPXr0YPDgwXWhcMUVV3DTTTfx6KOP1p2cBoiLi+P555/nsssuw+fzcdJJJ3HLLbeEZqeNaUuKd0DeQijaDntz4aunoOepcOnz4Ik+4ubhKGTDfYvIGOAz4Fug1t98H9AdQFWf8ofIY8B4oAy4XlVz/NufB/wN8ABTVfWPR3pPG+67ZdjP1ISVqjL44h+w4G9QXea0RXmh5+nwo39CbLKr5YVaQ8N9h/Iqps8Jfi4hcB0FflrPstnA7BCUZoyJdKqQvw42fgoL/u4MoDfwIjjlZ9C+OySkQ5TdR2xDbRhjIocqfPR7WPwSlO5y2jqfCBc/7czgZg4SEQGhqvVeBWSaJpxmIDQRaMVb8Nlfoe945+Rz1hjo0CvsL1c9WmEfEHFxcRQUFJCWlmYhcYxUlYKCAuLi4twuxZimK9nlXJXUdYQzwqon7D/+jlnY/4QyMzPJy8ujtd5E19bExcWRmZnpdhnGNI0qzLoTqkrhoictHBop7H9K0dHR9OzZ0+0yjDEtqeA7+OJRiIqGTgOhoghWz4KzH4KO/dyurs0I+4AwxkSQ8r0w/2H46mnnUtUoL1T5bxjNPAlOvtXd+toYCwhjTHhY9wHMuNmZtGfY1c58DEmdYN9m2L0GjhsGUR63q2xTLCCMMW1bjQ8+/iN8/gh0OgGueRu6BIyMnNrD+TJNZgFhjGlbinfCwmecw0m+Cti5ArYtgeHXwrl/huh4tysMGxYQxpi2QRWWvwmzf+mcdI5LAW+cMxTGxU/DkCvcrjDsWEAYY1q/ou3w3l2w6l3omu1cqtqxr9tVhT0LCGNM6+WrhP887lyZVFsNZz0IJ//M7mNoIfZTNsa0Tpu/hBm3wN6N0O98+P4fnGExTIuxgDDGtC6q/pnc7oGUbs5kPcef4XZVEckCwhjT8jZ+Buveh+py8JU7N7SldIP2PWDDJ7D0ZehzDlzyDMS3d7vaiGUBYYxpOaX5MPfX8M108MRAdIJzWWpNNZTlH1jvtLth7L02J4PLQjnl6FTgAmCXqp4QZPldwFUBdQwAOqrqHhHJBYqBGsBX32xHxpg2ZPW/4Z2fQmUJnHYXnPqLg+9ZqCqFfVucu53T+7hXp6kTyh7ECzjTib4YbKGq/gX4C4CI/AC4U1X3BKwyTlXzg21rjGljdnwLb9wAHfs79yxk9D98nZjE4O3GNaGccnS+iGQ1cvWJwLRQ1WKMcVFFIfzrWohrD1e9DkkZbldkGsn1A3wikgCMB94MaFbgfRFZJCKT3anMGHPMVOGdW2HvJrjseQuHNqY1nKT+AbDgkMNLo1V1m4hkAB+IyGpVnR9sY3+ATAbo3r176Ks1xjRO0TbnJrdVM515GHqc4nZFpolaQ0BcwSGHl1R1m//7LhGZAYwEggaEqk4BpgBkZ2fbhMnGuG3dB85kPRs/AxROvBxOuc3tqsxRcDUgRCQFOB24OqAtEYhS1WL/43OAh1wq0RjTFFsXw7SJ0K4LnH43DP4RpPd2uypzlEJ5mes0YCyQLiJ5wANANICqPuVf7WLgfVUtDdi0EzBDRPbX96qqzglVncaYo1S2B2LbHRgXqaIQ3rgekjvD5E8hoYO79ZljFsqrmCY2Yp0XcC6HDWzbAAwJTVXGmGOmCotfhPfudu58Pu8v0PM0ePd25z6G69+zcAgTreEchDGmragsgVl3wrf/gh5joHALvDjBme85byGc+QB0H+V2laaZWEAYY+pXVQbzfgcF651DSHs3OUNijPu1cyd0TRUs+Dt8/n/Q+ywYfYfbFZtmZAFhjKnfvN/BV0/BccOcG92yRkP2jdDzVGd5VDyMvQdG3QzRiTZ2UpixgDDGBLfhUyccRt4M5/254XXjU1umJtOiLO6NMYerKIS3fwJpvZ1Z3ExEsh6EMQY2zofXr4dOA50rknZ8C8Xb4cYPICbB7eqMSywgjIl0ZXvgrcnO0Ntle+CjPzjtp90FmSPcrc24ygLCmEimCjN/5kzkc9M86DLEebxzOWSd6nZ1xmUWEMZEskXPw+pZcM4fnXAASEyHXmNdLcu0DnaS2phItXURzLkPjj8DvvcTt6sxrZAFhDGRaNUseP58SOwIFz1p9y+YoOwQkzHhTBV2r4FdK53JepI6w9o58P5voOtwmDjdJvEx9bKAMCYcFW6Fr592egp7vjt8+YAJztzQdgmraYAFhDHhpnwv/PMHsG+Tc0/DyT9xBtMr2wMlOyHKC4MuscNK5ogsIIwJJzU+eH0S7NsMk/4N3b/ndkWmDbOAMCaczL0PNnwCFz5u4WCOmQWEMW1NTbXTQyje4QyHUbLLOXS0dyOsfAe+91MYdvWRX8eYIwjllKNTgQuAXap6QpDlY4F3gI3+prdU9SH/svHA3wEP8Kyq/ilUdRrTphRuhZcuhvw1B7d7YiAxA4ZdA2fbFO6meYSyB/EC8BjwYgPrfKaqFwQ2iIgHeBw4G8gDForITFVdGapCjWkT9m5yTj6X7YHzH4EOvSC5i3OZanwqOPO4G9NsQjkn9XwRyTqKTUcC6/1zUyMi04ELAQsIE7kKvnPCoaoErn3HBtEzLcLt69xOFpFlIvKeiAzyt3UFtgSsk+dvC0pEJotIjojk7N69O5S1GtPyamth0QvwzBngq4DrZlk4mBbj5knqxUAPVS0RkfOAt4E+QLB+stb3Iqo6BZgCkJ2dXe96xrQ5O1fCrDtgy1fQYwxMeBTSjne7KhNBXAsIVS0KeDxbRJ4QkXScHkO3gFUzgW0tXZ8xrlGFhc86l6zGJjtjJQ2ZaOcYTItzLSBEpDOwU1VVREbiHO4qAPYBfUSkJ7AVuAK40q06jWlRlcUw8zZY8Rb0OQcuegoS09yuykSoUF7mOg0YC6SLSB7wABANoKpPAZcCPxYRH1AOXKGqCvhE5FZgLs5lrlNVdUWo6jSmVagqhWXTYMGjULgFzrwfRt9pw2EYV4nzmRwesrOzNScnx+0yjGm8qlKY/zDkTIWKfXDccDjn95A1xu3KTIQQkUWqmh1smd1JbYxbti6CN2+CPRtg4ATnDuhuI+1cg2k1LCCMaUm1tZC/FlbMgM8ehqROzn0NvU53uzJjDmMBYUyoqcKa95z5GfIWQVWx0z7oErjgEecuaGNaIQsIY0JFFb77CD76A2xbDO17wJDLoesI6JoNHfu6XaExDbKAMKa5lRY4VyQtftEZVC+lG0x4zLmXwWP/5UzbYb+txjQXVfjyCfjwQaipcmZxm/AYnPgj8Ma6XZ0xTWYBYUxzqPHBe3c5l6v2Ow/O+A10GnTk7YxpxSwgjDlWZXvgzf+C7+bB6DvgzAfsBjcTFiwgjDlaRdvhP49BzvNQUwk/eBRGXOd2VcY0GwsIY5pKFT77K3z6v1BbAyf8EE79OWQMcLsyY5qVBYQxTeGrgndvh2WvwsCL4KwHoUNPt6syJiQsIIxprLI98K9rIfczGHsvnP4rGxbDhDULCGOOpKIQvnraOd9QXQ4XT3FueDMmzFlAGNOQnKnOfQ0VhdDvfBh3L3Qe7HZVxrQICwhjglGFj/8b5v8Zeo2Fs34Hxw11uypjWlSjAkJEEoFyVa0Vkb5Af+A9Va0OaXXGuKG2Fub8Cr6eAsOuhgv+bkNkmIjU2Lt55gNxItIVmAdcD7wQqqKMcU11Bbx1kxMOJ9/qDJVh4WAiVGMDQlS1DLgE+IeqXgwMbHADkakisktEltez/CoR+cb/9YWIDAlYlisi34rIUhGxKeJMyygtgBcvhOVvOHdDn/MHu0rJRLTG/mkkInIycBVwYyO3fQF4DHixnuUbgdNVda+InAtMAUYFLB+nqvmNrM+Yo6cKO5fDa9dA0Ta4dKpz85sxEa6xAXEHcC8wQ1VXiEgv4OOGNlDV+SKS1cDyLwKefglkNrIWY46dqjMc99o5kJcDpbsgIR0mzXKm/TTGNC4gVPVT4FMAEYkC8lX1tmas40bgvcC3BN4XEQWeVtUp9W0oIpOByQDdu3dvxpJM2Koqg5m3wvI3oUMv6H2mM4lP/wugXRe3qzOm1WjsVUyvArcANcAiIEVEHlHVvxxrASIyDicgxgQ0j1bVbSKSAXwgIqtVdX6w7f3hMQUgOztbj7UeE4b2boLKYmdqz5pKeH0SbP8GzrwfxvzczjMYU4/GHmIaqKpFInIVMBv4FU5QHFNAiMiJwLPAuapasL9dVbf5v+8SkRnASJwrqYxpmuVvOkNxa+2BtphkmDgN+p3rXl3GtAGNDYhoEYkGLgIeU9Vq/+GfoyYi3YG3gGtUdW1AeyIQparF/sfnAA8dy3uZCLV6Nrw1GbqfDKNudsZSqixyJvRJ7+N2dca0eo0NiKeBXGAZMF9EegBFDW0gItOAsUC6iOQBDwDRAKr6FHA/kAY8IU4X36eq2UAnYIa/zQu8qqpzmrRXxnz3Ebx+HXQZAle+BrHJbldkTJsjqkfXERARr6r6mrmeY5Kdna05OXbbREQr3glfPAoLn4W03nDdu5DQwe2qjGm1RGSR/4/zwzT2JHUKTg/gNH/TpziHfQqbpUJjjlVlCXz0B1j0PNRUweDL4Jw/WjgYcwwae4hpKrAc+JH/+TXA8zh3VhvjrsI8ePUK2LUChl7pXJmUdrzbVRnT5jU2II5X1cBbS38nIktDUI8xTbN1EUyb6NzbcOXr0OcstysyJmw0NiDKRWSMqn4OICKjgfLQlWXMEWxfBov+CUtfgaQMuPYdmxPamGbW2IC4BXjRfy4CYC9wXWhKMqYB6z+Eeb+H7UvBGweDLnYG1UtMd7syY8JOY4faWAYMEZF2/udFInIH8E0IazPmgOIdMOdeWPGWMzzGuX+GE3/k3B1tjAmJJg10r6qB9z78HPhbs1ZjTDCrZ8OMm8FXCeN+DaNvB2+s21UZE/aOZSYUG8DGhN6qd52xkzqfCD981q5OMqYFHUtA2MB4JrRWvgNv3ADHDYer34S4dm5XZExEaTAgRKSY4EEgQHxIKjKRrbIENi1wTkYvfA4ys+GqNywcjHFBgwGhqjaAjWkZFYXwwf2w5BWorQZvPAycABP+YeMoGeMSm43duG/dB/Du7VC8HUZc7wRDt+9BdJzblRkT0SwgjHtqqmHOPc7Aeh37w49egswRbldljPGzgDDuqCiEf10HGz6Gk291ZnezS1eNaVUsIEzL25sLr14OBevhwsdh2NVuV2SMCcICwrSciiJY8Df4zxPgjYFrZkDP0464mTHGHVGhemERmSoiu0RkeT3LRUQeFZH1IvKNiAwPWDZeRNb4l90TqhpNC/rmdXh0GHz2V+h/PtzyuYWDMa1cyAICeAEY38Dyc4E+/q/JwJMAIuIBHvcvHwhMFJGBIazThNq6D2HGZGcMpZs+hkufg/bd3a7KGHMEITvEpKrzRSSrgVUuBF5UZ87TL0WkvYh0AbKA9aq6AUBEpvvXXRmqWk0I5a937obOGAjXvg0xiW5XZIxpJDfPQXQFtgQ8z/O3BWsfVd+LiMhknB4I3bvbX6Wu2/K1c/lq58HO8+lXQpQHrnjVwsGYNsbNgAg22J820B6Uqk4BpgBkZ2fb+FBuqSiE934Fy6YdaItLcYbOuPYdSO3hXm3GmKPiZkDkAd0CnmcC24CYetpNa7XhU3j7J86d0KfdBZkjYccy2LUKBkyAnqe6XaEx5ii4GRAzgVv95xhGAYWqul1EdgN9RKQnsBW4ArjSxTpNfXyVMO8h+M9jkNYbbnzfGVwPoO857tZmjDlmIQsIEZkGjAXSRSQPeACIBlDVp4DZwHnAeqAMuN6/zCcitwJzAQ8wVVVXhKpOc5R2r4E3b4Qd30L2jc60nzEJbldljGlGobyKaeIRlivw03qWzcYJENMaLXvNGVwvJgEmTod+57pdkTEmBOxOatN4viqYex8sfAZ6jHHuZ0ju7HZVxpgQsYAwjbNvi3M/Q97XcMrP4MwHwWO/PsaEM/sfbo5s5Tsw82dQWwuXvQCDLna7ImNMC7CAMPUr3gmf/DcsesGZF/rS55zhMowxEcECwhxMFTZ+CjlTYfW/odYHo2+Hcb9xRmA1xkQMCwhzQG0N/PsXsOh5iE+FUbc4U4Cm93a7MmOMCywgjKO6HN78L1g9C0bfAWPvtTmhjYlwFhAGSvPhtath85dw7p9h1M1uV2SMaQUsICJZbS0seQk+uB+qy+DSqXDCJW5XZYxpJSwgIpEq5H4G837v3NfQYzSc/1fIGOB2ZcaYVsQCIpJUFsPil5wrlArWQUI6XPQkDJkIEmyUdWNMJLOAiBSFefDypbB7FWSeBBc9BYMuguh4tyszxrRSFhCRYPs38OqPoKoUrn4Lep/pdkXGmDbAAiKc1fhg+RvOvQ1xKXDDXOg00O2qjDFthAVEOKoqgyUvw3/+Afs2Q5ehMHEatDvO7cqMMW2IBUQ4qfHB0pfh4/+Bkh3O1J/j/wR9z4WoKLerM8a0MRYQ4WLt+/D+byB/jRMMl06FrNFuV2WMacNCGhAiMh74O87Uoc+q6p8OWX4XcFVALQOAjqq6R0RygWKgBvCpanYoa22zinfAe3c7Q3Kn9YbLX4b+F9hlq8aYYxbKOak9wOPA2UAesFBEZqrqyv3rqOpfgL/41/8BcKeq7gl4mXGqmh+qGts0VWcY7g8eAF8FnPEbOOV2G3HVGNNsQtmDGAmsV9UNACIyHbgQWFnP+hOBaSGsJ3wU5jkT+Hz3EfQ8DS74G6Qd73ZVxpgwE8qA6ApsCXieB4wKtqKIJADjgVsDmhV4X0QUeFpVp9Sz7WRgMkD37t2boexWTBWWTYP37nHmaTj/Eci+wQ4nGWNCIpQBEexTS+tZ9wfAgkMOL41W1W0ikgF8ICKrVXX+YS/oBMcUgOzs7Ppev+3LXwez7nTGUOp+Mlz0hM3uZowJqVAGRB7QLeB5JrCtnnWv4JDDS6q6zf99l4jMwDlkdVhAhL389c6Iq18+4QyLccH/wfBJdtmqMSbkQhkQC4E+ItIT2IoTAlceupKIpACnA1cHtCUCUapa7H98DvBQCGttXarLYeGz8M1rsONbQGDwZfD9P0JShtvVGWMiRMgCQlV9InIrMBfnMtepqrpCRG7xL3/Kv+rFwPuqWhqweSdghjjH1r3Aq6o6J1S1tirrPoTZv4S9G51B9b7/P86genYXtDGmhYlq+By2z87O1pycHLfLODrFO2D2XbBqJqT3deZn6Hma21UZY8KciCyq7z4zu5PabfuvTJpzD/gq4Yzfwim32f0MxhjXWUC4QRX2bIAtX8O3r8N385wrkyY8Bum93a7OGGMAC4iWVVkCXz0JXz0Npbudtrj2MP5/YeRkuzLJGNOqWEC0hIpCWPoqzH8YyvKh73jody50GwXp/SwYjDGtkgVEqBRuhXVzYdUs2Dgfaqudk85nPgCZNu6gMab1s4AAXv1qM2N6p9M9LeHYXmjrYlj+Jqyf58z9DJDaE753Cwy4ELqddOzFGmNMC4n4gNhXVsVzc/7D/2gKj/xoGGcP7HTkjWp8zjkEX7lz5VHeQlj4HGxfCp4Y6HEKDLsKep8FHfvbWEnGmDYp4gOifZyX92PupqgKvp7Wm3k9T2Zs9lA83miI8v94amtAa6DgO9i0ADZ/BdWlB79QxwFw3sNw4o+c+Z+NMaaNi/iAoNaH5+wHabfpS7LXfEba5n/A5gbW7zgAhk6EjIEQnQDRcZDSDbqOsJ6CMSasWEB4YyD7ejzZ15MGfJSzgmmfLmVzfhHHJXu5eFhXzhp0HAmxMZDUCRLT3K7YGGNahA21EYSq8tHqXTzxyXcs2rSX5Fgvl5/UjetOyaJbh2M8kW2MMa1IQ0NtWEAcwZLNe3l+QS6zv91OjSpn9MvgmpN7cFqfjkRF2SElY0zbZgHRDHYUVvDqV5t49est5JdU0iMtgStHdufSEZmkJcWG5D2NMSbULCCaUZWvlveWb+eVLzfzde4eYjxRfP+Ezlye3Y1Tjk+zXoUxpk2xgAiRdTuLeeWrzcxYspXC8moyU+O5dEQmlwzLPPab7owxpgVYQIRYRXUNc1fs4F85W1iwvgCAk7JSuWR4Jued0IWUhOgWr8kYYxrDtYAQkfHA33FmlHtWVf90yPKxwDvARn/TW6r6UGO2DaY1TBiUt7eMd5Zu483FeWzYXUqMJ4qx/Tpy0bCunNE/g7hoj6v1GWNMIFcCQkQ8wFrgbCAPZ47qiaq6MmCdscAvVfWCpm4bTGsIiP1UlW+3FvLO0m28u2wbu4orSYzxcM6gzlxwYhdO7dORGK+N4mqMcZdbM8qNBNar6gZ/EdOBC4EGP+SbYdtWQUQ4MbM9J2a2577zBvDlhgJmfbON2d/uYMaSrSTHeTl7YCfOH9yF0b3TrWdhjGl1QhkQXYEtAc/zgFFB1jtZRJYB23B6EyuasG2b4IkSRvdOZ3TvdH434QQWrM/n399uZ+6KHby1eCuJMR5O79eRcwZ2Zly/DDtnYYxpFUIZEMGu9zz0eNZioIeqlojIecDbQJ9Gbuu8ichkYDJA9+7dj7rYlhLjjWJc/wzG9c/gvy8ezILv8nl/xU4+XLWT2d/uwBMlZPdI5eyBnRjbryPHd0xCbIwnY4wLQnkO4mTgQVX9vv/5vQCq+j8NbJMLZOOERJO2hdZ1DqKpamuVpXn7mLdqJ/NW7WL1jmIAuqTEcWqfdMb06cgpx6eRbjflGWOakVsnqb04J5rPBLbinGi+0n8Iaf86nYGdqqoiMhJ4A+iBc+VSg9sG05YD4lB5e8v4bF0+89fuZsH6fIoqfAAM6NKOMb3TGNOnIyOzOhAfY+cujDFHz5WT1KrqE5Fbgbk4H/hTVXWFiNziX/4UcCnwYxHxAeXAFeokVtBtQ1Vra5SZmsDEkd2ZOLI7vppalm8rYsH6fD5fl88/v9jEM59tJMYTxdBu7RneI5XsHqkM75FKh8QYt0s3xoQJu1GuDSqvquHr3D18vm43X+fuZcXWQny1zr9jVloCw7s7YTGiRyp9OyXjseE/jDH1cOsyVxMi8TEeTu/bkdP7dgScO7mXbdnHki37WLxpL/PX5fPWkq0AJMV6GdIthaHd2jO0WypDuqWQkRznZvnGmDbCAiIMxEV7GNUrjVG9nMmMVJW8veUs2rSXnE17WLJ5H099uoEafy+jY3IsJxzXjkHHpXBCV+d7Zmq8XS1ljDmIBUQYEhG6dUigW4cELhrWFXAOS63YVsiyvEJWbCtk5bYi5q/LrwuNlPhoBnRJZkCXdgzo3I6+nZPpk5FEYqz9ihgTqex/f4SIj/GQndWB7KwOdW0V1TWs3lHMt1sLWbmtkJXbi5n29WYqqmvr1unaPp4+nZLok5FEn4xkCw5jIoj9L49gcdEe/7mJ9nVtNbXKpoJS1u0qYf2uEtbuLGbdzhL+810Blb4DwdGtQzy90pPomZ5IVloCPdIS6dYhgczUeBs2xJgwYQFhDuKJEnp1TKJXxyS+P+hAe02tsnlPGWt3FrN2RzFrdhazMb+UnNw9lFbV1K0nAl3axdEjLZGs9ESO75hI74wk+nRKpku7OJtQyZg2xALCNIonSuiZnkjP9ES+P6hzXbuqsrukki17ytm8p5RNBWVsLigjt6CUuSt2sKe0qm7dGG8Ume3j6ZoaT2ZqAj3SEujRIYHM1ATSk2NIS4y1EW6NaUUsIMwxEREykuPISI5jRI/Uw5bvKa2qO1S1ZU8ZeXvLydtbxtxtB4fHfqkJ0XRqF0endnEc1z7OH0pJZKUl0CkljuRYr11tZUwLsYAwIdUhMYaRPTswsmeHw5YVVVSzuaCMrfvKyS+pJL+4ip3FFewqqmRnUQXfbi08LEQSYjx0ahdHx6RYOibHkp4UQ0a7ODKSY8loF0fndnF0TomjXZwFiTHHygLCuKZdXDQndE3hhK4p9a5TWFbNxoJSNhWUsrOogh2FlewsrmB3cSWrdhSxu6iS4krfYdvFR3vomHwgRDomx9IxKY705Ji6cHGWxdpJdWPqYQFhWrWUhGiGJhx8pdWhyqtq2FVcwa7iSnYUVrCzqILthRVOr6Skko35pSzM3Rv0kBZAYoyHNH9odGoXS0ay0wvp7D/U1TE5hnbx0aTERxPrtTAxkcMCwrR58TEeeqQl0iMtscH1qmtq6w5l5ZdUsqu4goLSKgpKnOe7iytZs6OY+WvzKQnSKwHnEFd6ktMr6ZAYS2pCNKmJMaQmxBz0ODnOS1Ksl3Zx0STHee3qLdMmWUCYiBHtiaJLSjxdUuKPuG5Jpa+uN5JfUklReTWF5dXsKa2moNTpmeTtLWP51mr2llUddI/IoTxRQmpCDOlJMbRPiCY1IYb2/hBJjPGSGOshJd5pT030f09wei020KJxkwWEMUEkxXrpnZFE74ykRq1fVuVjb1k1e0ur2FdWTXFFNcWVPorKq9lTWsWe0ioKSqvYV1bFul0l7CuroqTSd9Bd64cScc7TpMRH0z4hmsQYLwkxHhJivSTHeWnvb2+fEEP7eKf3khIfTWKs03tJivVawJhjYgFhTDNIiPGSEOOla/sj904C+WpqKa2qobDM6YnsKXNCZG9pNfvKqtjn77kUlldTWuljR1E1ZVU1FFdUs6+sum6Y9/rr8jiHuuKj6eDvobSPjyEuOorYaA+x3igS/L2YxBhv3bmW/aGUEh9tJ/EjmAWEMS7yeqJIiY8iJT6a7mkJTdpWVSmp9LHPHy77yg4ESUmlj+IKX93jwnJnnY35pRSW76PSV0tldS0VvhqONCVMjDeKhBgPcV4PcdFRJMdF0y7eOb+SEOMlPiaK+GgP8TFekmO9JO7v4fgDJinW61/PQ3y0h2iP2CXIbYQFhDFtlIiQHBdNclw03To0LVz2U1UqqmsprfJRUuGjqOJAjyXwq6KqhvLqGsqraympqKaowseuohLKqmqoqK6hzL+8MaLEGQcsPtpDUsB5mLjoA1+x3ijioqOI8zqH1JJiPSTFRpMUdyCEEmOd3lFirJdoTxTeKMETJcR6oyyAmklIA0JExgN/x5k29FlV/dMhy68CfuV/WgL8WFWX+ZflAsVADeCrb8YjY8zRExHnL3v/1VnHoqZWKatyeixF5b66cCmprKa8qpayKh8V1TVUVNc6oVJd4/RwKnxOQFX6yC+porK6hkqfs065P3yatk/OfTD7Ayg5zkvy/nDxB0yMNwpPlBMqsd6ouhBKiPESH+1xekz+Ho/TO3K+EqI9eD2RMxxMyAJCRDzA48DZQB6wUERmqurKgNU2Aqer6l4ROReYAowKWD5OVfNDVaMxpvl4og70aLrUf+9jk9XWal2AOIfMaiipcJ6XVPooq/JRXaPU1Nbiq3V6RGWVvroAKq7wUVxRzZY9ZXXbVPmcdX01tRzhNM5hoj2CNyoKr0eI8UQ5vSF/mMRFR9X1ggLXCwya/ed8EmKcoIqOEryeqLpAS4r1EOv1EO2JItojxPq3deOCg1D2IEYC61V1A4CITAcuBOoCQlW/CFj/SyAzhPUYY9qgqIDgCYXqmlrKKmsorXLCZn9vp6y6hsqAXkx51YFDab6aWqprlOqaWsqrncNs5VVO76i4wsfu4sq6AKquUSp9zvLy6pomB9J+sd4oYrxRRIkQJeCJiiLW64RIRnIc/7rl5Ob9wRDagOgKbAl4nsfBvYND3Qi8F/BcgfdFRIGnVXVKsI1EZDIwGaB79+7HVLAxJvJEe6JISYgiJSE0ARRIVan01db1hqr9AeKrcQ7POT2lGqp8tVTX1FLlq6XSV0NppRMuVf77bWpV6wKqyldLYmxorjQLZUAE6w8FzU4RGYcTEGMCmker6jYRyQA+EJHVqjr/sBd0gmMKQHZ29lFmszHGhJ6I1B2COtZzPi0hlGdb8oBuAc8zgW2HriQiJwLPAheqasH+dlXd5v++C5iBc8jKGGNMCwllQCwE+ohITxGJAa4AZgauICLdgbeAa1R1bUB7oogk738MnAMsD2GtxhhjDhGyQ0yq6hORW4G5OJe5TlXVFSJyi3/5U8D9QBrwhP+65f2Xs3YCZvjbvMCrqjonVLUaY4w5nOiRbqNsQ7KzszUnJ8ftMowxps0QkUX13WcWOXd8GGOMaRILCGOMMUFZQBhjjAnKAsIYY0xQYXWSWkR2A5uasEk6EGljPUXiPkNk7nck7jNE5n4fyz73UNWOwRaEVUA0lYjkRNoosZG4zxCZ+x2J+wyRud+h2mc7xGSMMSYoCwhjjDFBRXpABB0hNsxF4j5DZO53JO4zROZ+h2SfI/ochDHGmPpFeg/CGGNMPSwgjDHGBBWRASEi40VkjYisF5F73K4nVESkm4h8LCKrRGSFiNzub+8gIh+IyDr/91S3a21uIuIRkSUiMsv/PBL2ub2IvCEiq/3/5ieH+36LyJ3+3+3lIjJNROLCcZ9FZKqI7BKR5QFt9e6niNzr/3xbIyLfP9r3jbiAEBEP8DhwLjAQmCgiA92tKmR8wC9UdQDwPeCn/n29B5inqn2Aef7n4eZ2YFXA80jY578Dc1S1PzAEZ//Ddr9FpCtwG5CtqifgTCtwBeG5zy8A4w9pC7qf/v/jVwCD/Ns84f/ca7KICwicmenWq+oGVa0CpgMXulxTSKjqdlVd7H9cjPOB0RVnf//pX+2fwEWuFBgiIpIJnI8zU+F+4b7P7YDTgOcAVLVKVfcR5vuNM19MvIh4gQScWSvDbp/90y3vOaS5vv28EJiuqpWquhFYz1HOyBmJAdEV2BLwPM/fFtZEJAsYBnwFdFLV7eCECJDhYmmh8DfgbqA2oC3c97kXsBt43n9o7Vn/bIxhu9+quhV4GNgMbAcKVfV9wnifD1HffjbbZ1wkBoQEaQvra31FJAl4E7hDVYvcrieUROQCYJeqLnK7lhbmBYYDT6rqMKCU8Di0Ui//MfcLgZ7AcUCiiFztblWtQrN9xkViQOQB3QKeZ+J0S8OSiETjhMMrqvqWv3mniHTxL+8C7HKrvhAYDUwQkVycw4dniMjLhPc+g/N7naeqX/mfv4ETGOG832cBG1V1t6pW48xvfwrhvc+B6tvPZvuMi8SAWAj0EZGeIhKDczJnpss1hYQ4k3o/B6xS1UcCFs0ErvM/vg54p6VrCxVVvVdVM1U1C+ff9iNVvZow3mcAVd0BbBGRfv6mM4GVhPd+bwa+JyIJ/t/1M3HOs4XzPgeqbz9nAleISKyI9AT6AF8f1TuoasR9AecBa4HvgF+7XU8I93MMTtfyG2Cp/+s8IA3nqod1/u8d3K41RPs/Fpjlfxz2+wwMBXL8/95vA6nhvt/A74DVwHLgJSA2HPcZmIZznqUap4dwY0P7Cfza//m2Bjj3aN/XhtowxhgTVCQeYjLGGNMIFhDGGGOCsoAwxhgTlAWEMcaYoCwgjDHGBGUBYUwTiEiNiCwN+Gq2u5VFJCtwtE5j3OZ1uwBj2phyVR3qdhHGtATrQRjTDEQkV0T+V0S+9n/19rf3EJF5IvKN/3t3f3snEZkhIsv8X6f4X8ojIs/45zh4X0TiXdspE/EsIIxpmvhDDjFdHrCsSFVHAo/hjCiL//GLqnoi8ArwqL/9UeBTVR2CM2bSCn97H+BxVR0E7AN+GNK9MaYBdie1MU0gIiWqmhSkPRc4Q1U3+AdI3KGqaSKSD3RR1Wp/+3ZVTReR3UCmqlYGvEYW8IE6E8AgIr8ColX1Dy2wa8YcxnoQxjQfredxfesEUxnwuAY7T2hcZAFhTPO5POD7f/yPv8AZVRbgKuBz/+N5wI+hbv7sdi1VpDGNZX+dGNM08SKyNOD5HFXdf6lrrIh8hfOH10R/223AVBG5C2fGt+v97bcDU0TkRpyewo9xRus0ptWwcxDGNAP/OYhsVc13uxZjmosdYjLGGBOU9SCMMcYEZT0IY4wxQVlAGGOMCcoCwhhjTFAWEMYYY4KygDDGGBPU/wN0DSi/KR50ygAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(1,len(train_losses) + 1)), train_losses, label = \"Training\")\n",
    "plt.plot(list(range(1,len(val_losses) +1)), val_losses, label = \"Validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.68    100973\n",
      "           1       0.69      0.58      0.63    101182\n",
      "\n",
      "    accuracy                           0.66    202155\n",
      "   macro avg       0.66      0.66      0.66    202155\n",
      "weighted avg       0.66      0.66      0.66    202155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = []\n",
    "labels = torch.tensor(list(y_val['label'].values))\n",
    "model = FFNN(num_layers, input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "with torch.no_grad():\n",
    "    for index, row in X_val.iterrows():\n",
    "        output = model(torch.Tensor(row))\n",
    "        preds.append(torch.argmax(output, dim=0))\n",
    "\n",
    "print(classification_report(labels, preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.69    101032\n",
      "           1       0.69      0.59      0.63    101123\n",
      "\n",
      "    accuracy                           0.66    202155\n",
      "   macro avg       0.67      0.66      0.66    202155\n",
      "weighted avg       0.67      0.66      0.66    202155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds = []\n",
    "labels = torch.tensor(list(y_test['label'].values))\n",
    "model = FFNN(num_layers, input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "with torch.no_grad():\n",
    "    for index, row in X_test.iterrows():\n",
    "        output = model(torch.Tensor(row))\n",
    "        preds.append(torch.argmax(output, dim=0))\n",
    "\n",
    "print(classification_report(labels, preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 LSTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class Indices(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_seq_idx_set = Indices(X_train_s)\n",
    "val_seq_idx_set = Indices(X_val_s)\n",
    "test_seq_idx_set = Indices(X_test_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "606463"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq_idx_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "500000"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq_idx_set[500000]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class SarcasmLSTM(nn.Module):\n",
    "    def __init__(self, input_size=300, lstm_hidden_size=256, lstm_num_layers=1, linear_hidden_size=256, linear_num_layers=1, output_size=2):\n",
    "        super(SarcasmLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.comment_lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True)\n",
    "        self.parent_comment_lstm = nn.LSTM(input_size=input_size, hidden_size=lstm_hidden_size, num_layers=lstm_num_layers, batch_first=True)\n",
    "        self.linear = nn.Sequential()\n",
    "        for i in range(linear_num_layers):\n",
    "            self.linear.append(nn.Linear(lstm_hidden_size * 2 if i == 0 else linear_hidden_size, linear_hidden_size))\n",
    "            self.linear.append(nn.ReLU())\n",
    "        self.linear.append(nn.Linear(lstm_hidden_size * 2 if linear_num_layers == 0 else linear_hidden_size, output_size))\n",
    "\n",
    "    def forward(self, comments, parent_comments):\n",
    "        packed_comments = nn.utils.rnn.pack_sequence(comments, enforce_sorted=False)\n",
    "        packed_parent_comments = nn.utils.rnn.pack_sequence(parent_comments, enforce_sorted=False)\n",
    "        packed_comments_encoding, _ = self.comment_lstm(packed_comments)\n",
    "        packed_parent_comments_encoding, _ = self.parent_comment_lstm(packed_parent_comments)\n",
    "        comments_encoding, comments_lengths = nn.utils.rnn.pad_packed_sequence(packed_comments_encoding, batch_first=True)\n",
    "        parent_comments_encoding, parent_comments_lengths = nn.utils.rnn.pad_packed_sequence(packed_parent_comments_encoding, batch_first=True)\n",
    "        comments_encoding = comments_encoding[range(comments_encoding.shape[0]), comments_lengths - 1]\n",
    "        parent_comments_encoding = parent_comments_encoding[range(parent_comments_encoding.shape[0]), parent_comments_lengths - 1]\n",
    "        encoding = torch.concat((comments_encoding, parent_comments_encoding), 1)\n",
    "        logit = self.linear(encoding)\n",
    "        return logit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class SarcasmLSTMDetector:\n",
    "    def __init__(self, lstm_model):\n",
    "        self.model = lstm_model\n",
    "        print(self.model)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert(array):\n",
    "        return torch.from_numpy(array)\n",
    "\n",
    "    def fit(self, train_indices, train_seq, train_labels, val_indices, val_seq, val_labels, lr=0.001, epochs=100):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_loss_min = math.inf\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "\n",
    "            self.model.train()\n",
    "            for indices in train_indices:\n",
    "                mini_batch = train_seq.iloc[indices]\n",
    "                comments = mini_batch['comment_seq']\n",
    "                comments = comments.map(SarcasmLSTMDetector.convert)\n",
    "                parent_comments = mini_batch['parent_comment_seq']\n",
    "                parent_comments = parent_comments.map(SarcasmLSTMDetector.convert)\n",
    "                labels = torch.from_numpy(train_labels.iloc[indices].to_numpy().reshape(-1))\n",
    "                predictions = self.model(comments, parent_comments)\n",
    "                loss = loss_fn(predictions, labels)\n",
    "                train_loss += loss.item()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "            for indices in val_indices:\n",
    "                mini_batch = val_seq.iloc[indices]\n",
    "                comments = mini_batch['comment_seq']\n",
    "                comments = comments.map(SarcasmLSTMDetector.convert)\n",
    "                parent_comments = mini_batch['parent_comment_seq']\n",
    "                parent_comments = parent_comments.map(SarcasmLSTMDetector.convert)\n",
    "                labels = torch.from_numpy(val_labels.iloc[indices].to_numpy().reshape(-1))\n",
    "                predictions = self.model(comments, parent_comments)\n",
    "                loss = loss_fn(predictions, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            train_losses.append(train_loss/len(trainloader))\n",
    "            val_losses.append(val_loss/len(valloader))\n",
    "            if val_loss <= val_loss_min:\n",
    "                torch.save(model.state_dict(), 'lstm_model.pt')\n",
    "                val_loss_min = val_loss\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(\"Epoch completed: \" + str(epoch+1) +\n",
    "                      f\"\\n\\tTraining Loss: {train_losses[epoch]:.3f}\" +\n",
    "                      f\"\\n\\tValidation Loss: {val_losses[epoch]:.3f}\\n\")\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def eval(self, test_indices, test_seq, test_labels):\n",
    "        test_loss = 0\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        self.model.eval()\n",
    "        for indices in test_indices:\n",
    "            mini_batch = test_seq.iloc[indices]\n",
    "            comments = mini_batch['comment_seq']\n",
    "            comments = comments.map(SarcasmLSTMDetector.convert)\n",
    "            parent_comments = mini_batch['parent_comment_seq']\n",
    "            parent_comments = parent_comments.map(SarcasmLSTMDetector.convert)\n",
    "            labels = torch.from_numpy(test_labels.iloc[indices].to_numpy().reshape(-1))\n",
    "            predictions = self.model(comments, parent_comments)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            test_loss += loss.item()\n",
    "        print(f\"Test Loss: {test_loss:.3f}\")\n",
    "        return test_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "lstm_batch_size = 64\n",
    "train_seq_idx_loader = torch.utils.data.DataLoader(train_seq_idx_set, lstm_batch_size, True)\n",
    "val_seq_idx_loader = torch.utils.data.DataLoader(val_seq_idx_set, lstm_batch_size, True)\n",
    "test_seq_idx_loader = torch.utils.data.DataLoader(test_seq_idx_set, lstm_batch_size, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SarcasmLSTM(\n",
      "  (comment_lstm): LSTM(300, 256, batch_first=True)\n",
      "  (parent_comment_lstm): LSTM(300, 256, batch_first=True)\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "baseline_model = SarcasmLSTM()\n",
    "baseline_detector = SarcasmLSTMDetector(baseline_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [204]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m lstm_train_losses, lstm_val_losses \u001B[38;5;241m=\u001B[39m \u001B[43mbaseline_detector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_seq_idx_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_seq_idx_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_seq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_labels\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [201]\u001B[0m, in \u001B[0;36mSarcasmLSTMDetector.fit\u001B[0;34m(self, train_indices, train_seq, train_labels, val_indices, val_seq, val_labels, lr, epochs)\u001B[0m\n\u001B[1;32m     28\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(train_labels\u001B[38;5;241m.\u001B[39miloc[indices]\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[1;32m     29\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(comments, parent_comments)\n\u001B[0;32m---> 30\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     32\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CS6140-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CS6140-Project/lib/python3.9/site-packages/torch/nn/modules/loss.py:1163\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/CS6140-Project/lib/python3.9/site-packages/torch/nn/functional.py:2996\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   2994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2995\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 2996\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "lstm_train_losses, lstm_val_losses = baseline_detector.fit(train_seq_idx_loader, train_seq, train_labels, val_seq_idx_loader, val_seq, val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 1, 1])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.iloc[[2,4,5,7]].to_numpy().reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "2    [[-0.1447100043296814, 0.2703399956226349, -0....\n4    [[-0.029205, 0.35396, -0.69416, 0.31261, 0.155...\n5    [[-0.41355, 0.076012, 0.099178, -0.01195, 0.01...\n7    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\nName: parent_comment_seq, dtype: object"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq.iloc[[2,4,5,7]]['parent_comment_seq']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Sarcasm Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}